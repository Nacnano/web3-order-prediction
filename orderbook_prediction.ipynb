{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6399adb",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Configuration\n",
    "Import necessary libraries and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1756d7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:36:52.916012Z",
     "iopub.status.busy": "2025-06-06T07:36:52.915454Z",
     "iopub.status.idle": "2025-06-06T07:37:10.302554Z",
     "shell.execute_reply": "2025-06-06T07:37:10.301755Z",
     "shell.execute_reply.started": "2025-06-06T07:36:52.915988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61493b",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "Set the path to your local order book dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f09101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:38:07.803334Z",
     "iopub.status.busy": "2025-06-06T07:38:07.802677Z",
     "iopub.status.idle": "2025-06-06T07:38:07.806814Z",
     "shell.execute_reply": "2025-06-06T07:38:07.806164Z",
     "shell.execute_reply.started": "2025-06-06T07:38:07.803306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "LOCAL_ORDERBOOK_FILEPATH = '/kaggle/input/bybit-orderbook/bybit_btcusdt_spot_orderbook_50_3600sec.json'\n",
    "\n",
    "# Feature Engineering & Modeling Parameters\n",
    "VAMP_LIQUIDITY_CUTOFF = 60000 # In dollars, as per PDF findings\n",
    "QUOTE_IMBALANCE_LEVELS = 5 # Number of LOB levels for QI (ensure data has this depth)\n",
    "PREDICTION_HORIZON_SECONDS = 60 # Predict price change 60 seconds ahead\n",
    "TARGET_TYPE = 'regression' # 'regression' or 'classification' (for direction: up/down/neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe762b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103003fb",
   "metadata": {},
   "source": [
    "### Method: Loading from a Local Dataset\n",
    "Load order book data from a CSV file. The dataset should contain time-series of LOB snapshots (e.g., best bid/ask, and deeper levels if available).\n",
    "Expected columns: `timestamp`, `best_bid_price`, `best_bid_qty`, `best_ask_price`, `best_ask_qty`, `bid_price_L2`, `bid_qty_L2`, ... `ask_price_L5`, `ask_qty_L5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd4662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:38:10.753367Z",
     "iopub.status.busy": "2025-06-06T07:38:10.752890Z",
     "iopub.status.idle": "2025-06-06T07:38:41.581618Z",
     "shell.execute_reply": "2025-06-06T07:38:41.580938Z",
     "shell.execute_reply.started": "2025-06-06T07:38:10.753345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(f\"Loading order book data from local file: {LOCAL_ORDERBOOK_FILEPATH}\")\n",
    "try:\n",
    "    # Read JSON file\n",
    "    with open(LOCAL_ORDERBOOK_FILEPATH, 'r') as file:\n",
    "        orderbook_data_list = json.load(file)\n",
    "    \n",
    "    # Initialize list to store processed data\n",
    "    data_list = []\n",
    "\n",
    "    # Process each order book update in the list\n",
    "    for orderbook_data in orderbook_data_list:\n",
    "        # Extract relevant data\n",
    "        timestamp_ms = orderbook_data['ts']\n",
    "        bids = orderbook_data['data']['b']\n",
    "        asks = orderbook_data['data']['a']\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        timestamp = pd.to_datetime(timestamp_ms, unit='ms')\n",
    "        \n",
    "        # Create a dictionary for this update\n",
    "        data_dict = {'timestamp': timestamp}\n",
    "        \n",
    "        # Process bids (descending order by price, filter out zero quantities)\n",
    "        bids = sorted([b for b in bids if float(b[1]) > 0], key=lambda x: float(x[0]), reverse=True)\n",
    "        for i, bid in enumerate(bids[:5], 1):  # Limit to 5 levels as per QUOTE_IMBALANCE_LEVELS\n",
    "            data_dict[f'bid_price_L{i}'] = float(bid[0])\n",
    "            data_dict[f'bid_qty_L{i}'] = float(bid[1])\n",
    "        \n",
    "        # Process asks (ascending order by price, filter out zero quantities)\n",
    "        asks = sorted([a for a in asks if float(a[1]) > 0], key=lambda x: float(x[0]))\n",
    "        for i, ask in enumerate(asks[:5], 1):  # Limit to 5 levels\n",
    "            data_dict[f'ask_price_L{i}'] = float(ask[0])\n",
    "            data_dict[f'ask_qty_L{i}'] = float(ask[1])\n",
    "        \n",
    "        # Set best bid and ask (first valid entries)\n",
    "        data_dict['best_bid_price'] = data_dict.get('bid_price_L1', None)\n",
    "        data_dict['best_bid_qty'] = data_dict.get('bid_qty_L1', None)\n",
    "        data_dict['best_ask_price'] = data_dict.get('ask_price_L1', None)\n",
    "        data_dict['best_ask_qty'] = data_dict.get('ask_qty_L1', None)\n",
    "        \n",
    "        data_list.append(data_dict)\n",
    "    \n",
    "    # Create DataFrame from all updates\n",
    "    df_lob_raw = pd.DataFrame(data_list).set_index('timestamp')\n",
    "    \n",
    "    # Ensure no duplicate timestamps (use last update if duplicates exist)\n",
    "    df_lob_raw = df_lob_raw[~df_lob_raw.index.duplicated(keep='last')]\n",
    "    \n",
    "    print(\"LOB data (raw head):\\n\", df_lob_raw.tail())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {LOCAL_ORDERBOOK_FILEPATH} not found.\")\n",
    "    df_lob_raw = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading local data: {e}. Falling back to empty DataFrame.\")\n",
    "    df_lob_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677e1c5",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preprocessing\n",
    "Resample the order book data to 1-second intervals to reduce data size, as mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d681c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:39:46.485111Z",
     "iopub.status.busy": "2025-06-06T07:39:46.484480Z",
     "iopub.status.idle": "2025-06-06T07:39:46.557775Z",
     "shell.execute_reply": "2025-06-06T07:39:46.556914Z",
     "shell.execute_reply.started": "2025-06-06T07:39:46.485086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not df_lob_raw.empty:\n",
    "    # Resample LOB data to 1-second frequency. Use last observation in interval.\n",
    "    agg_dict_lob = {}\n",
    "    if 'best_bid_price' in df_lob_raw.columns: agg_dict_lob['best_bid_price'] = 'last'\n",
    "    if 'best_bid_qty' in df_lob_raw.columns: agg_dict_lob['best_bid_qty'] = 'last'\n",
    "    if 'best_ask_price' in df_lob_raw.columns: agg_dict_lob['best_ask_price'] = 'last'\n",
    "    if 'best_ask_qty' in df_lob_raw.columns: agg_dict_lob['best_ask_qty'] = 'last'\n",
    "    for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "        if f'bid_price_L{i}' in df_lob_raw.columns: agg_dict_lob[f'bid_price_L{i}'] = 'last'\n",
    "        if f'bid_qty_L{i}' in df_lob_raw.columns: agg_dict_lob[f'bid_qty_L{i}'] = 'last'\n",
    "        if f'ask_price_L{i}' in df_lob_raw.columns: agg_dict_lob[f'ask_price_L{i}'] = 'last'\n",
    "        if f'ask_qty_L{i}' in df_lob_raw.columns: agg_dict_lob[f'ask_qty_L{i}'] = 'last'\n",
    "\n",
    "    if df_lob_raw.index.empty or not isinstance(df_lob_raw.index, pd.DatetimeIndex):\n",
    "        print(\"Warning: df_lob_raw has no DatetimeIndex. Cannot resample. Ensure 'timestamp' is index and datetime.\")\n",
    "        df_lob_sec = df_lob_raw.copy()\n",
    "    elif not agg_dict_lob:\n",
    "        print(\"Warning: No LOB columns found for aggregation in df_lob_raw.\")\n",
    "        df_lob_sec = pd.DataFrame(index=df_lob_raw.index.unique())\n",
    "    else:\n",
    "        df_lob_sec = df_lob_raw.resample('1S').agg(agg_dict_lob)\n",
    "    \n",
    "    if 'best_bid_price' in df_lob_sec.columns and 'best_ask_price' in df_lob_sec.columns:\n",
    "        df_lob_sec.dropna(subset=['best_bid_price', 'best_ask_price'], inplace=True)\n",
    "    print(\"Resampled LOB data (1-second frequency, head):\\n\", df_lob_sec.head())\n",
    "else:\n",
    "    print(\"df_lob_raw is empty. Skipping LOB resampling.\")\n",
    "    df_lob_sec = pd.DataFrame()\n",
    "\n",
    "df_combined = df_lob_sec.copy()\n",
    "\n",
    "if not df_combined.empty:\n",
    "    if 'best_bid_price' in df_combined.columns:\n",
    "        lob_cols = df_lob_sec.columns if not df_lob_sec.empty else []\n",
    "        lob_cols_to_ffill = [col for col in lob_cols if col in df_combined.columns]\n",
    "        if lob_cols_to_ffill:\n",
    "            df_combined[lob_cols_to_ffill] = df_combined[lob_cols_to_ffill].ffill().bfill()\n",
    "    \n",
    "    if 'best_bid_price' in df_combined.columns and 'best_ask_price' in df_combined.columns:\n",
    "        df_combined.dropna(subset=['best_bid_price', 'best_ask_price'], inplace=True)\n",
    "    else:\n",
    "        print(\"Warning: Essential LOB columns (best_bid_price, best_ask_price) are missing in df_combined. Further steps might fail.\")\n",
    "    print(\"Preprocessed data (head):\\n\", df_combined.head())\n",
    "else:\n",
    "    print(\"df_combined is empty after preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bf7c3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering\n",
    "Based on the paper \"Mind the Gaps\", focusing on order book features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1dc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:39:52.779035Z",
     "iopub.status.busy": "2025-06-06T07:39:52.778346Z",
     "iopub.status.idle": "2025-06-06T07:39:53.041374Z",
     "shell.execute_reply": "2025-06-06T07:39:53.040819Z",
     "shell.execute_reply.started": "2025-06-06T07:39:52.779009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure we have the necessary base columns\n",
    "if df_combined.empty or not all(col in df_combined.columns for col in ['best_bid_price', 'best_ask_price']):\n",
    "    print(\"Skipping feature engineering due to missing base LOB data (best_bid_price, best_ask_price) in df_combined.\")\n",
    "else:\n",
    "    # --- Mid-Price and Spread ---\n",
    "    df_combined['mid_price'] = (df_combined['best_bid_price'] + df_combined['best_ask_price']) / 2\n",
    "    df_combined['spread'] = df_combined['best_ask_price'] - df_combined['best_bid_price']\n",
    "\n",
    "    # --- Volume-Adjusted Mid-Price (VAMP) ---\n",
    "    def calculate_weighted_price_for_vamp(levels_prices, levels_qty, target_value_usd):\n",
    "        \"\"\" Helper for VAMP. Calculates weighted price for one side. \"\"\"\n",
    "        cumulative_value = 0\n",
    "        weighted_price_sum = 0\n",
    "        total_qty_for_value = 0\n",
    "        for price, qty in zip(levels_prices, levels_qty):\n",
    "            if pd.isna(price) or pd.isna(qty) or qty == 0 or price == 0:\n",
    "                continue\n",
    "            value_at_level = price * qty\n",
    "            if cumulative_value + value_at_level >= target_value_usd:\n",
    "                remaining_value_needed = target_value_usd - cumulative_value\n",
    "                if price == 0: continue\n",
    "                qty_to_take = remaining_value_needed / price\n",
    "                weighted_price_sum += price * qty_to_take\n",
    "                total_qty_for_value += qty_to_take\n",
    "                cumulative_value += remaining_value_needed\n",
    "                break\n",
    "            else:\n",
    "                weighted_price_sum += price * qty \n",
    "                total_qty_for_value += qty\n",
    "                cumulative_value += value_at_level\n",
    "        return (weighted_price_sum / total_qty_for_value) if total_qty_for_value > 0 else np.nan\n",
    "\n",
    "    vamp_bids_p_cols = [f'bid_price_L{i}' for i in range(1, QUOTE_IMBALANCE_LEVELS + 1) if f'bid_price_L{i}' in df_combined.columns]\n",
    "    vamp_bids_q_cols = [f'bid_qty_L{i}' for i in range(1, QUOTE_IMBALANCE_LEVELS + 1) if f'bid_qty_L{i}' in df_combined.columns]\n",
    "    vamp_asks_p_cols = [f'ask_price_L{i}' for i in range(1, QUOTE_IMBALANCE_LEVELS + 1) if f'ask_price_L{i}' in df_combined.columns]\n",
    "    vamp_asks_q_cols = [f'ask_qty_L{i}' for i in range(1, QUOTE_IMBALANCE_LEVELS + 1) if f'ask_qty_L{i}' in df_combined.columns]\n",
    "\n",
    "    if vamp_bids_p_cols and vamp_bids_q_cols and vamp_asks_p_cols and vamp_asks_q_cols and \\\n",
    "       len(vamp_bids_p_cols) == len(vamp_bids_q_cols) and len(vamp_asks_p_cols) == len(vamp_asks_q_cols):\n",
    "        \n",
    "        Pb_vamp_values = []\n",
    "        Pa_vamp_values = []\n",
    "        for index, row in df_combined.iterrows():\n",
    "            bid_prices = [row[col] for col in vamp_bids_p_cols]\n",
    "            bid_qtys = [row[col] for col in vamp_bids_q_cols]\n",
    "            ask_prices = [row[col] for col in vamp_asks_p_cols]\n",
    "            ask_qtys = [row[col] for col in vamp_asks_q_cols]\n",
    "            \n",
    "            Pb_vamp_values.append(calculate_weighted_price_for_vamp(bid_prices, bid_qtys, VAMP_LIQUIDITY_CUTOFF))\n",
    "            Pa_vamp_values.append(calculate_weighted_price_for_vamp(ask_prices, ask_qtys, VAMP_LIQUIDITY_CUTOFF))\n",
    "            \n",
    "        df_combined['Pb_vamp'] = Pb_vamp_values\n",
    "        df_combined['Pa_vamp'] = Pa_vamp_values\n",
    "        df_combined['vamp'] = (df_combined['Pb_vamp'] + df_combined['Pa_vamp']) / 2\n",
    "        df_combined['vamp_mid_diff'] = df_combined['mid_price'] - df_combined['vamp']\n",
    "    else:\n",
    "        df_combined['vamp'] = df_combined['mid_price']\n",
    "        df_combined['vamp_mid_diff'] = 0\n",
    "        print(\"Warning: VAMP calculation could not be performed due to missing/mismatched deep LOB data. Using mid_price as fallback.\")\n",
    "\n",
    "    # --- Quote Imbalance (QI) ---\n",
    "    sum_bid_qty_L = pd.Series(0.0, index=df_combined.index)\n",
    "    sum_ask_qty_L = pd.Series(0.0, index=df_combined.index)\n",
    "    qi_cols_found = False\n",
    "    for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "        bid_col_name = f'bid_qty_L{i}'\n",
    "        ask_col_name = f'ask_qty_L{i}'\n",
    "        if bid_col_name in df_combined.columns and ask_col_name in df_combined.columns:\n",
    "            sum_bid_qty_L += df_combined[bid_col_name].fillna(0)\n",
    "            sum_ask_qty_L += df_combined[ask_col_name].fillna(0)\n",
    "            qi_cols_found = True\n",
    "            \n",
    "    if qi_cols_found:\n",
    "        numerator_qi = sum_bid_qty_L - sum_ask_qty_L\n",
    "        denominator_qi = sum_bid_qty_L + sum_ask_qty_L\n",
    "        df_combined['quote_imbalance'] = (numerator_qi / denominator_qi.replace(0, np.nan)).fillna(0)\n",
    "        df_combined['quote_imbalance'] = np.clip(df_combined['quote_imbalance'], -1, 1)\n",
    "    else:\n",
    "        df_combined['quote_imbalance'] = 0\n",
    "        print(\"Warning: Quote Imbalance calculation skipped due to missing LOB quantity columns.\")\n",
    "\n",
    "    print(\"Data with engineered features (head):\\n\", df_combined[['mid_price', 'spread', 'vamp', 'vamp_mid_diff', 'quote_imbalance']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c5218",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Target Variable Creation\n",
    "Predict future price changes using the mid-price. The paper uses look-ahead windows from 1s to 60s.\n",
    "Target: $MidPrice_{t+\\Delta t} - MidPrice_t$ (absolute change) or direction for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d83ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:40:07.676822Z",
     "iopub.status.busy": "2025-06-06T07:40:07.676542Z",
     "iopub.status.idle": "2025-06-06T07:40:07.901158Z",
     "shell.execute_reply": "2025-06-06T07:40:07.900616Z",
     "shell.execute_reply.started": "2025-06-06T07:40:07.676802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 'mid_price' not in df_combined.columns or df_combined.empty:\n",
    "    print(\"Skipping target variable creation as mid_price is missing or df_combined is empty.\")\n",
    "    df_final = pd.DataFrame()\n",
    "else:\n",
    "    df_combined['future_mid_price'] = df_combined['mid_price'].shift(-PREDICTION_HORIZON_SECONDS)\n",
    "    \n",
    "    if TARGET_TYPE == 'regression':\n",
    "        df_combined['target_price_change'] = df_combined['future_mid_price'] - df_combined['mid_price']\n",
    "        TARGET_COLUMN = 'target_price_change'\n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        price_diff = df_combined['future_mid_price'] - df_combined['mid_price']\n",
    "        if 'spread' in df_combined.columns and not df_combined['spread'].isnull().all():\n",
    "            neutral_threshold = df_combined['spread'].mean() * 0.1\n",
    "        else:\n",
    "            neutral_threshold = 0.001\n",
    "            print(f\"Warning: 'spread' column missing or all NaN. Using fixed neutral_threshold: {neutral_threshold}\")\n",
    "        df_combined['target_direction'] = 1\n",
    "        df_combined.loc[price_diff > neutral_threshold, 'target_direction'] = 2\n",
    "        df_combined.loc[price_diff < -neutral_threshold, 'target_direction'] = 0\n",
    "        TARGET_COLUMN = 'target_direction'\n",
    "\n",
    "    df_final = df_combined.dropna(subset=[TARGET_COLUMN])\n",
    "    \n",
    "    if not df_final.empty:\n",
    "        print(f\"Final data with target variable '{TARGET_COLUMN}' (head):\\n\", df_final[['mid_price', 'future_mid_price', TARGET_COLUMN] + [col for col in ['vamp_mid_diff', 'quote_imbalance'] if col in df_final.columns]].head())\n",
    "        print(f\"\\nTarget variable ({TARGET_COLUMN}) distribution:\")\n",
    "        if TARGET_TYPE == 'regression':\n",
    "            df_final[TARGET_COLUMN].plot(kind='hist', bins=50, title='Target Price Change Distribution')\n",
    "            plt.show()\n",
    "            print(df_final[TARGET_COLUMN].describe())\n",
    "        elif TARGET_TYPE == 'classification':\n",
    "            print(df_final[TARGET_COLUMN].value_counts(normalize=True))\n",
    "    else:\n",
    "        print(\"DataFrame is empty after target creation and NaN removal. Check data or prediction horizon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7b6a8",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Stationarity Checks and Transformations\n",
    "Ensure features are stationary for financial modeling. Check features like `vamp_mid_diff` and `quote_imbalance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f49cf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:40:12.022322Z",
     "iopub.status.busy": "2025-06-06T07:40:12.022016Z",
     "iopub.status.idle": "2025-06-06T07:40:12.433565Z",
     "shell.execute_reply": "2025-06-06T07:40:12.432878Z",
     "shell.execute_reply.started": "2025-06-06T07:40:12.022298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_stationarity(series, series_name=''):\n",
    "    \"\"\"Performs ADF test and prints results.\"\"\"\n",
    "    if not isinstance(series, pd.Series) or series.empty or series.isnull().all():\n",
    "        print(f\"Series {series_name} is not a valid Series, is empty, or all NaN. Skipping stationarity check.\")\n",
    "        return False\n",
    "    print(f'\\nStationarity Test for {series_name}:')\n",
    "    try:\n",
    "        series_cleaned = series.dropna().astype(float)\n",
    "        if series_cleaned.empty:\n",
    "            print(f\"Series {series_name} is empty after dropna. Skipping stationarity check.\")\n",
    "            return False\n",
    "        result = adfuller(series_cleaned)\n",
    "        print('ADF Statistic: %f' % result[0])\n",
    "        print('p-value: %f' % result[1])\n",
    "        if result[1] <= 0.05:\n",
    "            print(f\"Result: Likely Stationary (p-value <= 0.05) for {series_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Result: Likely Non-Stationary (p-value > 0.05) for {series_name}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error during stationarity test for {series_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "feature_columns_for_model = []\n",
    "df_model_ready = pd.DataFrame()\n",
    "\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    potential_features = ['spread', 'vamp_mid_diff', 'quote_imbalance']\n",
    "    df_transformed = df_final.copy()\n",
    "\n",
    "    for col in potential_features:\n",
    "        if col in df_transformed.columns:\n",
    "            if not check_stationarity(df_transformed[col], col):\n",
    "                print(f\"Feature {col} is non-stationary. Applying differencing.\")\n",
    "                df_transformed[f'{col}_diff'] = df_transformed[col].diff()\n",
    "                if check_stationarity(df_transformed[f'{col}_diff'].dropna(), f'{col}_diff'):\n",
    "                    feature_columns_for_model.append(f'{col}_diff')\n",
    "                else:\n",
    "                    print(f\"Differenced feature {col}_diff is still non-stationary. Consider further transformation or excluding.\")\n",
    "            else:\n",
    "                feature_columns_for_model.append(col)\n",
    "    \n",
    "    if TARGET_TYPE == 'regression' and TARGET_COLUMN in df_transformed.columns:\n",
    "        check_stationarity(df_transformed[TARGET_COLUMN], TARGET_COLUMN)\n",
    "    \n",
    "    if feature_columns_for_model and TARGET_COLUMN in df_transformed.columns:\n",
    "        final_cols_to_select = [f for f in feature_columns_for_model if f in df_transformed.columns] + [TARGET_COLUMN]\n",
    "        df_model_ready = df_transformed[final_cols_to_select].copy()\n",
    "        df_model_ready.dropna(inplace=True)\n",
    "    else:\n",
    "        print(\"Warning: No features selected or target column missing after stationarity check.\")\n",
    "\n",
    "    if df_model_ready.empty:\n",
    "        print(\"DataFrame for modeling (df_model_ready) is empty after dropping NaNs. Check data, feature engineering, and stationarity steps.\")\n",
    "    else:\n",
    "        print(f\"\\nFeatures selected for modeling: {feature_columns_for_model}\")\n",
    "        print(f\"Shape of df_model_ready: {df_model_ready.shape}\")\n",
    "else:\n",
    "    print(\"Skipping stationarity checks as df_final is not available or empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8efd1",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af0325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T07:40:24.089808Z",
     "iopub.status.busy": "2025-06-06T07:40:24.089528Z",
     "iopub.status.idle": "2025-06-06T07:40:40.017975Z",
     "shell.execute_reply": "2025-06-06T07:40:40.017280Z",
     "shell.execute_reply.started": "2025-06-06T07:40:24.089789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 'df_model_ready' not in locals() or df_model_ready.empty or not feature_columns_for_model:\n",
    "    print(\"Skipping model building as data is not ready or no features are selected.\")\n",
    "else:\n",
    "    X = df_model_ready[feature_columns_for_model]\n",
    "    y = df_model_ready[TARGET_COLUMN]\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(\"X or y is empty. Cannot proceed with model training.\")\n",
    "    else:\n",
    "        train_size_pct = 0.8\n",
    "        split_idx = int(len(X) * train_size_pct)\n",
    "        \n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "        if X_train.empty or X_test.empty:\n",
    "            print(\"Training or testing set is empty after split. Check data size and split point.\")\n",
    "        else:\n",
    "            print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model_results = {}\n",
    "\n",
    "            def evaluate_model(name, model, X_test_data, y_true, y_pred):\n",
    "                if TARGET_TYPE == 'regression':\n",
    "                    mse = mean_squared_error(y_true, y_pred)\n",
    "                    r2 = r2_score(y_true, y_pred)\n",
    "                    print(f\"{name} - MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "                    model_results[name] = {'MSE': mse, 'R2': r2}\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.scatter(y_true, y_pred, alpha=0.5, label='Predicted vs Actual')\n",
    "                    min_val = min(y_true.min(), y_pred.min()) if not y_true.empty and not pd.Series(y_pred).empty else 0\n",
    "                    max_val = max(y_true.max(), y_pred.max()) if not y_true.empty and not pd.Series(y_pred).empty else 1\n",
    "                    plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')\n",
    "                    plt.xlabel(\"Actual Values\")\n",
    "                    plt.ylabel(\"Predicted Values\")\n",
    "                    plt.title(f\"{name} - Predictions vs Actuals\")\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                elif TARGET_TYPE == 'classification':\n",
    "                    accuracy = accuracy_score(y_true, y_pred)\n",
    "                    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "                    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "                    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "                    print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "                    model_results[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}\n",
    "\n",
    "            # --- Model Implementations ---\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                print(\"\\n--- Linear Regression ---\")\n",
    "                model_lr = LinearRegression()\n",
    "                model_lr.fit(X_train_scaled, y_train)\n",
    "                y_pred_lr = model_lr.predict(X_test_scaled)\n",
    "                evaluate_model(\"Linear Regression\", model_lr, X_test_scaled, y_test, y_pred_lr)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                print(\"\\n--- Logistic Regression ---\")\n",
    "                model_logr = LogisticRegression(solver='liblinear', multi_class='auto', random_state=42, max_iter=1000)\n",
    "                model_logr.fit(X_train_scaled, y_train)\n",
    "                y_pred_logr = model_logr.predict(X_test_scaled)\n",
    "                evaluate_model(\"Logistic Regression\", model_logr, X_test_scaled, y_test, y_pred_logr)\n",
    "\n",
    "            print(\"\\n--- Decision Tree ---\")\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_dt = DecisionTreeRegressor(random_state=42, max_depth=10, min_samples_split=10)\n",
    "                model_dt.fit(X_train_scaled, y_train)\n",
    "                y_pred_dt = model_dt.predict(X_test_scaled)\n",
    "                evaluate_model(\"Decision Tree Regressor\", model_dt, X_test_scaled, y_test, y_pred_dt)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                model_dtc = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=10)\n",
    "                model_dtc.fit(X_train_scaled, y_train)\n",
    "                y_pred_dtc = model_dtc.predict(X_test_scaled)\n",
    "                evaluate_model(\"Decision Tree Classifier\", model_dtc, X_test_scaled, y_test, y_pred_dtc)\n",
    "\n",
    "            print(\"\\n--- Random Forest ---\")\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10, n_jobs=-1)\n",
    "                model_rf.fit(X_train_scaled, y_train)\n",
    "                y_pred_rf = model_rf.predict(X_test_scaled)\n",
    "                evaluate_model(\"Random Forest Regressor\", model_rf, X_test_scaled, y_test, y_pred_rf)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                model_rfc = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10, n_jobs=-1)\n",
    "                model_rfc.fit(X_train_scaled, y_train)\n",
    "                y_pred_rfc = model_rfc.predict(X_test_scaled)\n",
    "                evaluate_model(\"Random Forest Classifier\", model_rfc, X_test_scaled, y_test, y_pred_rfc)\n",
    "\n",
    "            print(\"\\n--- XGBoost ---\")\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42, max_depth=7, learning_rate=0.1, n_jobs=-1)\n",
    "                model_xgb_reg.fit(X_train_scaled, y_train)\n",
    "                y_pred_xgb_reg = model_xgb_reg.predict(X_test_scaled)\n",
    "                evaluate_model(\"XGBoost Regressor\", model_xgb_reg, X_test_scaled, y_test, y_pred_xgb_reg)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                num_class_xgb = len(np.unique(y_train))\n",
    "                objective_xgb_clf = 'multi:softmax' if num_class_xgb > 2 else 'binary:logistic'\n",
    "                model_xgbc_params = {'n_estimators': 100, 'random_state': 42, 'max_depth': 7, 'learning_rate': 0.1, 'n_jobs': -1, 'objective': objective_xgb_clf}\n",
    "                if objective_xgb_clf == 'multi:softmax': model_xgbc_params['num_class'] = num_class_xgb\n",
    "                model_xgbc = xgb.XGBClassifier(**model_xgbc_params)\n",
    "                model_xgbc.fit(X_train_scaled, y_train)\n",
    "                y_pred_xgbc = model_xgbc.predict(X_test_scaled)\n",
    "                evaluate_model(\"XGBoost Classifier\", model_xgbc, X_test_scaled, y_test, y_pred_xgbc)\n",
    "                \n",
    "            print(\"\\n--- Support Vector Machine (SVM) ---\")\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "                model_svr.fit(X_train_scaled, y_train)\n",
    "                y_pred_svr = model_svr.predict(X_test_scaled)\n",
    "                evaluate_model(\"SVR\", model_svr, X_test_scaled, y_test, y_pred_svr)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                model_svc = SVC(kernel='rbf', C=1.0, random_state=42, probability=True)\n",
    "                model_svc.fit(X_train_scaled, y_train)\n",
    "                y_pred_svc = model_svc.predict(X_test_scaled)\n",
    "                evaluate_model(\"SVC\", model_svc, X_test_scaled, y_test, y_pred_svc)\n",
    "\n",
    "            print(\"\\n--- Neural Network (MLP) ---\")\n",
    "            def create_mlp(input_dim, num_unique_targets=1, classification=False):\n",
    "                model = Sequential()\n",
    "                model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(32, activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                if classification:\n",
    "                    if num_unique_targets <= 2:\n",
    "                         model.add(Dense(1, activation='sigmoid'))\n",
    "                         model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                    else:\n",
    "                         model.add(Dense(num_unique_targets, activation='softmax'))\n",
    "                         model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                else:\n",
    "                    model.add(Dense(1, activation='linear'))\n",
    "                    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "                return model\n",
    "\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_mlp_reg = create_mlp(X_train_scaled.shape[1], classification=False)\n",
    "                early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "                model_mlp_reg.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stop], verbose=0)\n",
    "                y_pred_mlp_reg = model_mlp_reg.predict(X_test_scaled).flatten()\n",
    "                evaluate_model(\"MLP Regressor\", model_mlp_reg, X_test_scaled, y_test, y_pred_mlp_reg)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                num_unique_targets_nn = len(np.unique(y_train))\n",
    "                model_mlpc = create_mlp(X_train_scaled.shape[1], num_unique_targets=num_unique_targets_nn, classification=True)\n",
    "                early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "                model_mlpc.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stop], verbose=0)\n",
    "                y_pred_mlpc_proba = model_mlpc.predict(X_test_scaled)\n",
    "                if num_unique_targets_nn <= 2:\n",
    "                    y_pred_mlpc = (y_pred_mlpc_proba > 0.5).astype(int).flatten()\n",
    "                else:\n",
    "                    y_pred_mlpc = np.argmax(y_pred_mlpc_proba, axis=1)\n",
    "                evaluate_model(\"MLP Classifier\", model_mlpc, X_test_scaled, y_test, y_pred_mlpc)\n",
    "\n",
    "            print(\"\\n--- Model Performance Summary ---\")\n",
    "            results_df = pd.DataFrame(model_results).T\n",
    "            print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7602615,
     "sourceId": 12077449,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
