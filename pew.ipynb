{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short-term Crypto Price Prediction based on Order Book Dynamics\n",
    "\n",
    "This notebook aims to implement and explore methods for short-term cryptocurrency price prediction, drawing inspiration from the research paper \"Mind the Gaps: Short-term Crypto Price Prediction\".\n",
    "\n",
    "## Objective\n",
    "Conduct quantitative research on order book dynamics and build predictive models to forecast future prices.\n",
    "\n",
    "## Scope of Work\n",
    "1. Download/Fetch order book data.\n",
    "2. Engineer features from the order book.\n",
    "3. Create ML/statistical models to predict future price changes.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Configuration\n",
    "2. Data Acquisition\n",
    "   - Method 1: Fetching from Bybit API\n",
    "   - Method 2: Loading from a local dataset\n",
    "3. Data Preprocessing\n",
    "   - LOB Reconstruction (if necessary)\n",
    "   - Resampling to Second-Level Data\n",
    "   - Cleaning\n",
    "4. Feature Engineering\n",
    "   - Mid-Price and Spread\n",
    "   - Volume-Adjusted Mid-Price (VAMP)\n",
    "   - Trade Imbalance (TI)\n",
    "   - Quote Imbalance (QI)\n",
    "5. Target Variable Creation\n",
    "6. Stationarity Checks and Transformations\n",
    "7. Model Building and Evaluation\n",
    "   - Data Splitting\n",
    "   - Linear Regression\n",
    "   - Logistic Regression (for classification)\n",
    "   - Decision Tree\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - Support Vector Machine (SVM)\n",
    "   - Neural Network (MLP)\n",
    "8. Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Configuration\n",
    "Import necessary libraries and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ccxt\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "Set your API keys for Bybit here if you choose Method 1.\n",
    "Set the path to your local dataset if you choose Method 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATA_SOURCE_METHOD = 'local_dataset' # 'bybit_api' or 'local_dataset'\n",
    "\n",
    "# For Bybit API (Method 1)\n",
    "BYBIT_API_KEY = 'YOUR_API_KEY'\n",
    "BYBIT_SECRET_KEY = 'YOUR_SECRET_KEY'\n",
    "BYBIT_SYMBOL = 'BTC/USDT' # Example symbol\n",
    "BYBIT_LOB_DEPTH = 25 # Number of levels for order book depth\n",
    "BYBIT_TRADE_LIMIT = 1000 # Number of recent trades to fetch\n",
    "\n",
    "# For Local Dataset (Method 2)\n",
    "# Assume CSV with columns like: timestamp, best_bid_price, best_bid_qty, best_ask_price, best_ask_qty, ...\n",
    "# Or trade data: timestamp, price, volume, side\n",
    "LOCAL_ORDERBOOK_FILEPATH = 'path/to/your/orderbook_data.csv'\n",
    "LOCAL_TRADES_FILEPATH = 'path/to/your/trades_data.csv'\n",
    "\n",
    "# Feature Engineering & Modeling Parameters\n",
    "VAMP_LIQUIDITY_CUTOFF = 60000 # In dollars, as per PDF findings\n",
    "TRADE_IMBALANCE_WINDOW = '60S' # 60 seconds for TI calculation\n",
    "QUOTE_IMBALANCE_LEVELS = 5 # Number of LOB levels for QI\n",
    "PREDICTION_HORIZON_SECONDS = 60 # Predict price change 60 seconds ahead\n",
    "TARGET_TYPE = 'regression' # 'regression' or 'classification' (for direction: up/down/neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Fetching from Bybit API (using `ccxt`)\n",
    "This section contains functions to fetch Limit Order Book (LOB) data and trade data from Bybit.\n",
    "\n",
    "**Note**: Live data fetching can be complex due to rate limits, data consistency, and the need for continuous streaming for a proper LOB reconstruction. The following provides a snapshot approach. For high-frequency analysis, a dedicated WebSocket connection and data storage solution would be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bybit_order_book(symbol, depth):\n",
    "    \"\"\"Fetches the current order book snapshot from Bybit.\"\"\"\n",
    "    exchange = ccxt.bybit({\n",
    "        'apiKey': BYBIT_API_KEY,\n",
    "        'secret': BYBIT_SECRET_KEY,\n",
    "        # 'enableRateLimit': True, # Optional: built-in rate limiting\n",
    "    })\n",
    "    try:\n",
    "        order_book = exchange.fetch_l2_order_book(symbol, limit=depth)\n",
    "        timestamp = order_book['timestamp']\n",
    "        bids = pd.DataFrame(order_book['bids'], columns=['price', 'qty'])\n",
    "        bids['side'] = 'bid'\n",
    "        asks = pd.DataFrame(order_book['asks'], columns=['price', 'qty'])\n",
    "        asks['side'] = 'ask'\n",
    "        \n",
    "        # Add cumulative volume for VAMP calculation later\n",
    "        bids = bids.sort_values('price', ascending=False).reset_index(drop=True)\n",
    "        asks = asks.sort_values('price', ascending=True).reset_index(drop=True)\n",
    "        bids['cumulative_qty'] = bids['qty'].cumsum()\n",
    "        bids['cumulative_value_usd'] = (bids['price'] * bids['qty']).cumsum() # Assuming price is in USDT for BTC/USDT\n",
    "        asks['cumulative_qty'] = asks['qty'].cumsum()\n",
    "        asks['cumulative_value_usd'] = (asks['price'] * asks['qty']).cumsum()\n",
    "\n",
    "        return bids, asks, timestamp\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Bybit order book for {symbol}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def fetch_bybit_trades(symbol, limit):\n",
    "    \"\"\"Fetches recent trades from Bybit.\"\"\"\n",
    "    exchange = ccxt.bybit({\n",
    "        'apiKey': BYBIT_API_KEY,\n",
    "        'secret': BYBIT_SECRET_KEY,\n",
    "    })\n",
    "    try:\n",
    "        trades = exchange.fetch_trades(symbol, limit=limit)\n",
    "        df_trades = pd.DataFrame(trades)\n",
    "        if not df_trades.empty:\n",
    "            df_trades['timestamp'] = pd.to_datetime(df_trades['timestamp'], unit='ms')\n",
    "            df_trades = df_trades[['timestamp', 'price', 'amount', 'side', 'cost']] # 'cost' is amount * price\n",
    "            df_trades.rename(columns={'amount': 'qty', 'cost': 'value_usd'}, inplace=True)\n",
    "            return df_trades.sort_values('timestamp').reset_index(drop=True)\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Bybit trades for {symbol}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage for Bybit API (if DATA_SOURCE_METHOD is 'bybit_api')\n",
    "if DATA_SOURCE_METHOD == 'bybit_api':\n",
    "    print(\"Attempting to fetch data from Bybit API...\")\n",
    "    # For a real analysis, you'd call these functions repeatedly and store the data.\n",
    "    # This is a single snapshot example.\n",
    "    bids_df, asks_df, lob_timestamp = fetch_bybit_order_book(BYBIT_SYMBOL, BYBIT_LOB_DEPTH)\n",
    "    trades_df_api = fetch_bybit_trades(BYBIT_SYMBOL, BYBIT_TRADE_LIMIT)\n",
    "\n",
    "    if bids_df is not None and asks_df is not None:\n",
    "        print(f\"\\nOrder Book Snapshot at {datetime.datetime.fromtimestamp(lob_timestamp/1000)}:\")\n",
    "        print(\"Top 5 Bids:\\n\", bids_df.head())\n",
    "        print(\"Top 5 Asks:\\n\", asks_df.head())\n",
    "    if not trades_df_api.empty:\n",
    "        print(\"\\nRecent Trades:\\n\", trades_df_api.head())\n",
    "    # For a full analysis, you would need to aggregate these snapshots over time\n",
    "    # or use websockets to stream data. The PDF uses historical tick data.\n",
    "    # Here, we'll create a placeholder for subsequent processing steps.\n",
    "    # In a real scenario, you'd build up 'all_order_book_data' and 'all_trades_data' over time.\n",
    "    all_order_book_data_raw = [] # List to store (timestamp, bids_df, asks_df) tuples\n",
    "    all_trades_data_raw = trades_df_api # Placeholder\n",
    "\n",
    "    # For demonstration, let's assume we collected a few snapshots\n",
    "    # This part needs a proper data collection loop for a real application\n",
    "    if bids_df is not None: # Add the single snapshot\n",
    "        all_order_book_data_raw.append({'timestamp': lob_timestamp, 'bids': bids_df, 'asks': asks_df})\n",
    "    # Create a combined DataFrame from these snapshots (simplified for demo)\n",
    "    # This is where you would structure your collected LOB snapshots into a usable format.\n",
    "    # For now, it's hard to proceed with features without continuous LOB data.\n",
    "    # The PDF used \"end of day snapshots\" and \"quote by quote updates\"\n",
    "    # which are then processed.\n",
    "\n",
    "    # To proceed with a single snapshot for feature calculation demonstration (limited use):\n",
    "    # best_bid_price = bids_df['price'].iloc[0] if not bids_df.empty else np.nan\n",
    "    # best_ask_price = asks_df['price'].iloc[0] if not asks_df.empty else np.nan\n",
    "    # This is insufficient for time-series modeling.\n",
    "    print(\"\\nNote: Live API fetching requires a continuous data collection strategy for robust analysis.\")\n",
    "    print(\"The current API example provides a single snapshot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Loading from a Local Dataset\n",
    "Load data from a CSV file. The PDF uses three months of tick-level data from Bitstamp.\n",
    "You'll need to adapt the loading based on your dataset's format.\n",
    "\n",
    "For this example, we'll assume two files:\n",
    "1.  `orderbook_data.csv`: Contains time-series of LOB snapshots (e.g., best bid/ask, and deeper levels if available).\n",
    "    Columns might be: `timestamp`, `best_bid_price`, `best_bid_qty`, `best_ask_price`, `best_ask_qty`, `bid_price_L2`, `bid_qty_L2`, ... `ask_price_L5`, `ask_qty_L5`.\n",
    "2.  `trades_data.csv`: Contains historical trades.\n",
    "    Columns might be: `timestamp`, `price`, `qty`, `side` (e.g., 'buy' or 'sell')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SOURCE_METHOD == 'local_dataset':\n",
    "    print(f\"Loading data from local files: {LOCAL_ORDERBOOK_FILEPATH} and {LOCAL_TRADES_FILEPATH}\")\n",
    "    try:\n",
    "        # --- Load Order Book Data ---\n",
    "        # This is a simplified loading. Your actual data might be structured differently.\n",
    "        # For example, it might be tick-by-tick updates that you need to reconstruct into LOB snapshots.\n",
    "        # The PDF mentions starting with end-of-day snapshots and then using quote updates.\n",
    "        # For simplicity, let's assume 'orderbook_data.csv' is already somewhat processed\n",
    "        # to have best bid/ask at each timestamp, and potentially deeper levels for QI.\n",
    "        # If you have raw LOB data (price levels and quantities), you'll need more complex parsing.\n",
    "        \n",
    "        # Placeholder: Create sample data if files don't exist, to allow the notebook to run.\n",
    "        # In a real scenario, replace this with actual data loading.\n",
    "        sample_timestamps = pd.to_datetime(pd.date_range(start='2023-01-01', periods=10000, freq='100ms').values)\n",
    "        \n",
    "        # Sample Order Book Data (replace with your actual data loading)\n",
    "        data_lob = {\n",
    "            'timestamp': sample_timestamps,\n",
    "            'best_bid_price': np.random.uniform(20000, 20100, 10000),\n",
    "            'best_bid_qty': np.random.uniform(0.1, 5, 10000),\n",
    "            'best_ask_price': np.random.uniform(20101, 20200, 10000),\n",
    "            'best_ask_qty': np.random.uniform(0.1, 5, 10000),\n",
    "        }\n",
    "        # Add deeper levels for Quote Imbalance\n",
    "        for i in range(1, QUOTE_IMBALANCE_LEVELS + 1): # L1 to L5\n",
    "            data_lob[f'bid_price_L{i}'] = data_lob['best_bid_price'] - i * np.random.uniform(0.5, 2, 10000)\n",
    "            data_lob[f'bid_qty_L{i}'] = np.random.uniform(0.1, 3, 10000)\n",
    "            data_lob[f'ask_price_L{i}'] = data_lob['best_ask_price'] + i * np.random.uniform(0.5, 2, 10000)\n",
    "            data_lob[f'ask_qty_L{i}'] = np.random.uniform(0.1, 3, 10000)\n",
    "\n",
    "        # Ensure asks are higher than bids\n",
    "        data_lob['best_ask_price'] = np.maximum(data_lob['best_ask_price'], data_lob['best_bid_price'] + 0.01)\n",
    "        for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "            data_lob[f'ask_price_L{i}'] = np.maximum(data_lob[f'ask_price_L{i}'], data_lob[f'bid_price_L{i}'] + (i*0.1))\n",
    "\n",
    "\n",
    "        df_lob_raw = pd.DataFrame(data_lob)\n",
    "        df_lob_raw['timestamp'] = pd.to_datetime(df_lob_raw['timestamp'])\n",
    "        df_lob_raw.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # --- Load Trades Data ---\n",
    "        # Sample Trades Data (replace with your actual data loading)\n",
    "        data_trades = {\n",
    "            'timestamp': sample_timestamps, # Align with LOB for simplicity here\n",
    "            'price': (df_lob_raw['best_bid_price'] + df_lob_raw['best_ask_price']) / 2,\n",
    "            'qty': np.random.uniform(0.01, 1, 10000),\n",
    "            'side': np.random.choice(['buy', 'sell'], 10000),\n",
    "        }\n",
    "        df_trades_raw = pd.DataFrame(data_trades)\n",
    "        df_trades_raw['timestamp'] = pd.to_datetime(df_trades_raw['timestamp'])\n",
    "        df_trades_raw['value_usd'] = df_trades_raw['price'] * df_trades_raw['qty']\n",
    "        df_trades_raw.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        print(\"Sample LOB data (raw head):\\n\", df_lob_raw.head())\n",
    "        print(\"Sample Trades data (raw head):\\n\", df_trades_raw.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: One or both data files not found. Please check paths: {LOCAL_ORDERBOOK_FILEPATH}, {LOCAL_TRADES_FILEPATH}\")\n",
    "        df_lob_raw = pd.DataFrame()\n",
    "        df_trades_raw = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading local data: {e}\")\n",
    "        df_lob_raw = pd.DataFrame()\n",
    "        df_trades_raw = pd.DataFrame()\n",
    "else:\n",
    "    print(\"Invalid DATA_SOURCE_METHOD selected.\")\n",
    "    df_lob_raw = pd.DataFrame()\n",
    "    df_trades_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preprocessing\n",
    "The PDF mentions condensing the full dataset to consider full seconds rather than every tick update to reduce data size.\n",
    "We will resample the data to 1-second intervals.\n",
    "\n",
    "**Note on LOB Reconstruction for VAMP/QI from Tick Data:**\n",
    "If `df_lob_raw` is from tick-by-tick updates (not snapshots), you'd need a more sophisticated LOB reconstruction process before this step. You would maintain the state of the order book at each tick and then sample it every second. The PDF mentions: \"Starting with the end of day snapshot, we then used each quote update to first update the order book, and then calculated and stored the important features\".\n",
    "For simplicity, if using `local_dataset`, we assume `df_lob_raw` provides snapshots or already has necessary levels available at each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_lob_raw.empty:\n",
    "    # Resample LOB data to 1-second frequency. Use last observation in interval.\n",
    "    # Adjust aggregation methods as needed (e.g., 'ohlc' for prices, 'sum' for quantities if meaningful)\n",
    "    # For features like best_bid/ask, 'last' is often appropriate.\n",
    "    # The PDF uses \"full seconds throughout the day\".\n",
    "    \n",
    "    agg_dict_lob = {}\n",
    "    # Best bid/ask\n",
    "    if 'best_bid_price' in df_lob_raw.columns: agg_dict_lob['best_bid_price'] = 'last'\n",
    "    if 'best_bid_qty' in df_lob_raw.columns: agg_dict_lob['best_bid_qty'] = 'last' # Or 'sum' if ticks are updates within the second\n",
    "    if 'best_ask_price' in df_lob_raw.columns: agg_dict_lob['best_ask_price'] = 'last'\n",
    "    if 'best_ask_qty' in df_lob_raw.columns: agg_dict_lob['best_ask_qty'] = 'last' # Or 'sum'\n",
    "    \n",
    "    # Deeper levels for QI\n",
    "    for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "        if f'bid_price_L{i}' in df_lob_raw.columns: agg_dict_lob[f'bid_price_L{i}'] = 'last'\n",
    "        if f'bid_qty_L{i}' in df_lob_raw.columns: agg_dict_lob[f'bid_qty_L{i}'] = 'last' # Or 'sum'\n",
    "        if f'ask_price_L{i}' in df_lob_raw.columns: agg_dict_lob[f'ask_price_L{i}'] = 'last'\n",
    "        if f'ask_qty_L{i}' in df_lob_raw.columns: agg_dict_lob[f'ask_qty_L{i}'] = 'last' # Or 'sum'\n",
    "\n",
    "    df_lob_sec = df_lob_raw.resample('1S').agg(agg_dict_lob) if agg_dict_lob else pd.DataFrame()\n",
    "    df_lob_sec.dropna(subset=['best_bid_price', 'best_ask_price'], inplace=True) # Ensure core data is present\n",
    "    print(\"Resampled LOB data (1-second frequency, head):\\n\", df_lob_sec.head())\n",
    "else:\n",
    "    df_lob_sec = pd.DataFrame()\n",
    "\n",
    "if not df_trades_raw.empty:\n",
    "    # Resample trades data to 1-second frequency\n",
    "    # Sum quantities and values, VWAP for price\n",
    "    agg_dict_trades = {\n",
    "        'price': 'last', # Could also be VWAP: lambda x: (x * df_trades_raw.loc[x.index, 'qty']).sum() / df_trades_raw.loc[x.index, 'qty'].sum() if not x.empty else np.nan\n",
    "        'qty': 'sum',\n",
    "        'value_usd': 'sum'\n",
    "    }\n",
    "    df_trades_sec = df_trades_raw.resample('1S').agg(agg_dict_trades)\n",
    "    \n",
    "    # For trade imbalance, we need buy/sell volumes separately\n",
    "    buy_volume_sec = df_trades_raw[df_trades_raw['side'] == 'buy']['qty'].resample('1S').sum().rename('buy_qty_sum')\n",
    "    sell_volume_sec = df_trades_raw[df_trades_raw['side'] == 'sell']['qty'].resample('1S').sum().rename('sell_qty_sum')\n",
    "    buy_value_sec = df_trades_raw[df_trades_raw['side'] == 'buy']['value_usd'].resample('1S').sum().rename('buy_value_sum')\n",
    "    sell_value_sec = df_trades_raw[df_trades_raw['side'] == 'sell']['value_usd'].resample('1S').sum().rename('sell_value_sum')\n",
    "\n",
    "    df_trades_info_sec = pd.concat([df_trades_sec, buy_volume_sec, sell_volume_sec, buy_value_sec, sell_value_sec], axis=1)\n",
    "    df_trades_info_sec.fillna({'buy_qty_sum': 0, 'sell_qty_sum': 0, 'buy_value_sum':0, 'sell_value_sum':0}, inplace=True)\n",
    "\n",
    "    print(\"Resampled Trades data (1-second frequency, head):\\n\", df_trades_info_sec.head())\n",
    "else:\n",
    "    df_trades_info_sec = pd.DataFrame()\n",
    "\n",
    "# Combine LOB and Trades data based on timestamp (outer join to keep all data points)\n",
    "df_combined = pd.DataFrame()\n",
    "if not df_lob_sec.empty and not df_trades_info_sec.empty:\n",
    "    df_combined = pd.merge(df_lob_sec, df_trades_info_sec, left_index=True, right_index=True, how='outer')\n",
    "elif not df_lob_sec.empty:\n",
    "    df_combined = df_lob_sec\n",
    "elif not df_trades_info_sec.empty:\n",
    "    df_combined = df_trades_info_sec\n",
    "\n",
    "# Forward fill missing LOB data (common for non-trading seconds), then backfill\n",
    "if 'best_bid_price' in df_combined.columns: # Check if LOB columns exist\n",
    "    lob_cols_to_ffill = [col for col in df_lob_sec.columns if col in df_combined.columns]\n",
    "    df_combined[lob_cols_to_ffill] = df_combined[lob_cols_to_ffill].ffill().bfill()\n",
    "\n",
    "# Fill NaNs for trade volumes/values with 0 for calculation purposes\n",
    "trade_qty_val_cols = ['qty', 'value_usd', 'buy_qty_sum', 'sell_qty_sum', 'buy_value_sum', 'sell_value_sum']\n",
    "for col in trade_qty_val_cols:\n",
    "    if col in df_combined.columns:\n",
    "        df_combined[col].fillna(0, inplace=True)\n",
    "\n",
    "df_combined.dropna(subset=['best_bid_price', 'best_ask_price'], inplace=True) # Critical for mid-price\n",
    "print(\"Combined and preprocessed data (head):\\n\", df_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering\n",
    "Based on the paper \"Mind the Gaps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have the necessary base columns\n",
    "if df_combined.empty or not all(col in df_combined.columns for col in ['best_bid_price', 'best_ask_price']):\n",
    "    print(\"Skipping feature engineering due to missing base LOB data (best_bid_price, best_ask_price).\")\n",
    "else:\n",
    "    # --- Mid-Price and Spread ---\n",
    "    df_combined['mid_price'] = (df_combined['best_bid_price'] + df_combined['best_ask_price']) / 2\n",
    "    df_combined['spread'] = df_combined['best_ask_price'] - df_combined['best_bid_price']\n",
    "\n",
    "    # --- Volume-Adjusted Mid-Price (VAMP) ---\n",
    "    # VAMP_v = (Pb + Pa) / 2\n",
    "    # Pb = sum(Pbi * Vbi) / v  (volume-weighted bid price up to cumulative volume v)\n",
    "    # Pa = sum(Pai * Vai) / v  (volume-weighted ask price up to cumulative volume v)\n",
    "    # 'v' is volume in dollars (VAMP_LIQUIDITY_CUTOFF)\n",
    "    # This requires having multiple levels of LOB data or reconstructing it.\n",
    "    # For this example, we'll assume 'df_lob_raw' (if from API) or the loaded CSV\n",
    "    # contains sufficient levels to calculate this.\n",
    "    # If using the simplified 'local_dataset' with only best_bid/ask, VAMP calculation will be trivial or impossible.\n",
    "    # The sample data created for 'local_dataset' earlier includes L1-L5, which is not enough depth typically for VAMP_LIQUIDITY_CUTOFF of $60k.\n",
    "    # A proper VAMP calculation requires summing up volume at different price levels until 'v' (e.g., $60k) is reached on both bid and ask sides.\n",
    "    # The formula from the paper: $P_{b}=\\frac{\\sum_{i}^{n}P_{b_i}\\times V_{b_i}}{v}$ (where $V_{b_i}$ is in base currency qty, $v$ is target value in quote)\n",
    "    # $P_{b} = \\frac{\\sum (PriceLevel_{bid,i} \\times VolumeAtPriceLevel_{bid,i})}{\\sum VolumeAtPriceLevel_{bid,i}}$ where $\\sum (PriceLevel_{bid,i} \\times VolumeAtPriceLevel_{bid,i})$ approx equals $v$.\n",
    "    # More accurately: $P_b$ is the weighted average price of bids one would need to transact to fill an order of value $v$.\n",
    "    # This is complex to implement without full LOB snapshot data structure per row of df_combined.\n",
    "    # Placeholder for VAMP - a proper implementation needs full LOB access per row of df_combined.\n",
    "    # For now, let's assume we have columns like 'bid_price_L{i}', 'bid_qty_L{i}', etc.\n",
    "    \n",
    "    def calculate_weighted_price_for_vamp(levels_prices, levels_qty, target_value_usd):\n",
    "        \"\"\" Helper for VAMP. Calculates weighted price for one side. \"\"\"\n",
    "        cumulative_value = 0\n",
    "        weighted_price_sum = 0\n",
    "        total_qty_for_value = 0\n",
    "        \n",
    "        for price, qty in zip(levels_prices, levels_qty):\n",
    "            if pd.isna(price) or pd.isna(qty) or qty == 0:\n",
    "                continue\n",
    "            \n",
    "            value_at_level = price * qty\n",
    "            if cumulative_value + value_at_level >= target_value_usd:\n",
    "                remaining_value_needed = target_value_usd - cumulative_value\n",
    "                qty_to_take = remaining_value_needed / price\n",
    "                weighted_price_sum += price * qty_to_take\n",
    "                total_qty_for_value += qty_to_take\n",
    "                cumulative_value += remaining_value_needed\n",
    "                break\n",
    "            else:\n",
    "                weighted_price_sum += price * qty\n",
    "                total_qty_for_value += qty\n",
    "                cumulative_value += value_at_level\n",
    "        \n",
    "        return (weighted_price_sum / total_qty_for_value) if total_qty_for_value > 0 else np.nan\n",
    "\n",
    "    # This VAMP is illustrative and depends heavily on having granular LOB data available in df_combined\n",
    "    # The sample data does not have enough depth for a $60k VAMP typically.\n",
    "    # You'd need to parse your actual LOB data to provide these lists of prices/quantities for each timestamp.\n",
    "    # For demonstration, using available L1-L5 from sample data:\n",
    "    vamp_bids_p, vamp_bids_q, vamp_asks_p, vamp_asks_q = [], [], [], []\n",
    "    for i in range(1, QUOTE_IMBALANCE_LEVELS + 1): # Using up to L5 from sample\n",
    "        if f'bid_price_L{i}' in df_combined.columns: vamp_bids_p.append(df_combined[f'bid_price_L{i}'])\n",
    "        if f'bid_qty_L{i}' in df_combined.columns: vamp_bids_q.append(df_combined[f'bid_qty_L{i}'])\n",
    "        if f'ask_price_L{i}' in df_combined.columns: vamp_asks_p.append(df_combined[f'ask_price_L{i}'])\n",
    "        if f'ask_qty_L{i}' in df_combined.columns: vamp_asks_q.append(df_combined[f'ask_qty_L{i}'])\n",
    "\n",
    "    if vamp_bids_p and vamp_bids_q and vamp_asks_p and vamp_asks_q:\n",
    "        # Transpose to iterate row-wise for apply\n",
    "        vamp_bids_p_T = pd.concat(vamp_bids_p, axis=1).transpose()\n",
    "        vamp_bids_q_T = pd.concat(vamp_bids_q, axis=1).transpose()\n",
    "        vamp_asks_p_T = pd.concat(vamp_asks_p, axis=1).transpose()\n",
    "        vamp_asks_q_T = pd.concat(vamp_asks_q, axis=1).transpose()\n",
    "\n",
    "        Pb_vamp_list = []\n",
    "        Pa_vamp_list = []\n",
    "        for idx in df_combined.index:\n",
    "            if idx not in vamp_bids_p_T.columns: continue # If timestamp missing after resample\n",
    "            bid_prices_at_ts = vamp_bids_p_T[idx].values\n",
    "            bid_qtys_at_ts = vamp_bids_q_T[idx].values\n",
    "            ask_prices_at_ts = vamp_asks_p_T[idx].values\n",
    "            ask_qtys_at_ts = vamp_asks_q_T[idx].values\n",
    "            \n",
    "            Pb_vamp_list.append(calculate_weighted_price_for_vamp(bid_prices_at_ts, bid_qtys_at_ts, VAMP_LIQUIDITY_CUTOFF))\n",
    "            Pa_vamp_list.append(calculate_weighted_price_for_vamp(ask_prices_at_ts, ask_qtys_at_ts, VAMP_LIQUIDITY_CUTOFF))\n",
    "        \n",
    "        if Pb_vamp_list and Pa_vamp_list:\n",
    "             # Align indices if there were skipped timestamps\n",
    "            valid_indices = [idx for idx in df_combined.index if idx in vamp_bids_p_T.columns][:len(Pb_vamp_list)]\n",
    "            df_combined.loc[valid_indices, 'Pb_vamp'] = Pb_vamp_list\n",
    "            df_combined.loc[valid_indices, 'Pa_vamp'] = Pa_vamp_list\n",
    "            df_combined['vamp'] = (df_combined['Pb_vamp'] + df_combined['Pa_vamp']) / 2\n",
    "            df_combined['vamp_mid_diff'] = df_combined['mid_price'] - df_combined['vamp'] # As used in paper's plots\n",
    "        else:\n",
    "            df_combined['vamp'] = df_combined['mid_price'] # Fallback\n",
    "            df_combined['vamp_mid_diff'] = 0\n",
    "\n",
    "    else: # Fallback if not enough LOB levels\n",
    "        df_combined['vamp'] = df_combined['mid_price']\n",
    "        df_combined['vamp_mid_diff'] = 0\n",
    "        print(\"Warning: VAMP calculation could not be performed due to missing deep LOB data. Using mid_price as fallback for VAMP.\")\n",
    "\n",
    "\n",
    "    # --- Trade Imbalance (TI) ---\n",
    "    # TI = sum(Y * (Vbuy - Vsell)) / sum(Y * (Vbuy + Vsell)) over past 1 min (60s)\n",
    "    # Y is linear weight, closer time has larger weight.\n",
    "    # For simplicity, let's use equal weights for now (rolling sum).\n",
    "    # The paper uses $Y_i$ as a linear weight. For a 60s window, weights could be 1/60, 2/60, ..., 60/60.\n",
    "    # Or, more simply, recent trades get higher weight. Or just sum over window.\n",
    "    # Let's use value (USD) for Vbuy and Vsell, assuming 'buy_value_sum' and 'sell_value_sum' are from df_trades_info_sec\n",
    "    \n",
    "    if 'buy_value_sum' in df_combined.columns and 'sell_value_sum' in df_combined.columns:\n",
    "        rolling_window_size_ti = int(pd.Timedelta(TRADE_IMBALANCE_WINDOW).total_seconds()) # Ensure integer for rolling\n",
    "        \n",
    "        # Numerator: Sum of (buy_value - sell_value)\n",
    "        diff_value = df_combined['buy_value_sum'] - df_combined['sell_value_sum']\n",
    "        # Denominator: Sum of (buy_value + sell_value)\n",
    "        total_value = df_combined['buy_value_sum'] + df_combined['sell_value_sum']\n",
    "\n",
    "        # Applying linear weights ( exemplu: 1 to N for N window size)\n",
    "        weights = np.arange(1, rolling_window_size_ti + 1)\n",
    "\n",
    "        # Weighted rolling sum\n",
    "        # Note: Pandas rolling().apply() can be slow. For performance, custom Cython or Numba might be needed for large datasets.\n",
    "        # For simplicity, if not applying complex weights, a simple .sum() is faster.\n",
    "        # Let's use simple sum for now as weighted rolling sum is more complex with standard pandas.\n",
    "        # To implement linear weights as in the paper, you'd need a custom rolling function or a library that supports weighted rolling sums efficiently.\n",
    "        # E.g., (diff_value.rolling(window=rolling_window_size_ti).apply(lambda x: np.sum(weights[:len(x)] * x) / np.sum(weights[:len(x)]), raw=True))\n",
    "\n",
    "        numerator_ti = diff_value.rolling(window=rolling_window_size_ti, min_periods=1).sum()\n",
    "        denominator_ti = total_value.rolling(window=rolling_window_size_ti, min_periods=1).sum()\n",
    "        \n",
    "        df_combined['trade_imbalance'] = numerator_ti / denominator_ti\n",
    "        df_combined['trade_imbalance'].fillna(0, inplace=True) # Fill cases where denominator is 0\n",
    "        df_combined['trade_imbalance'] = np.clip(df_combined['trade_imbalance'], -1, 1) # TI is -1 to 1\n",
    "    else:\n",
    "        df_combined['trade_imbalance'] = 0\n",
    "        print(\"Warning: Trade Imbalance calculation skipped due to missing trade value columns.\")\n",
    "\n",
    "\n",
    "    # --- Quote Imbalance (QI) ---\n",
    "    # QI_L=k = (sum(Vb_i) - sum(Va_i)) / (sum(Vb_i) + sum(Va_i)) for i=1 to k levels\n",
    "    # Using quantities at different LOB levels.\n",
    "    sum_bid_qty_L = pd.Series(0.0, index=df_combined.index)\n",
    "    sum_ask_qty_L = pd.Series(0.0, index=df_combined.index)\n",
    "\n",
    "    for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "        if f'bid_qty_L{i}' in df_combined.columns:\n",
    "            sum_bid_qty_L += df_combined[f'bid_qty_L{i}'].fillna(0)\n",
    "        if f'ask_qty_L{i}' in df_combined.columns:\n",
    "            sum_ask_qty_L += df_combined[f'ask_qty_L{i}'].fillna(0)\n",
    "            \n",
    "    if not sum_bid_qty_L.empty and not sum_ask_qty_L.empty:\n",
    "        numerator_qi = sum_bid_qty_L - sum_ask_qty_L\n",
    "        denominator_qi = sum_bid_qty_L + sum_ask_qty_L\n",
    "        df_combined['quote_imbalance'] = numerator_qi / denominator_qi\n",
    "        df_combined['quote_imbalance'].fillna(0, inplace=True) # Fill cases where denominator is 0\n",
    "        df_combined['quote_imbalance'] = np.clip(df_combined['quote_imbalance'], -1, 1) # QI is -1 to 1\n",
    "    else:\n",
    "        df_combined['quote_imbalance'] = 0\n",
    "        print(\"Warning: Quote Imbalance calculation skipped due to missing LOB quantity columns.\")\n",
    "\n",
    "    print(\"Data with engineered features (head):\\n\", df_combined[['mid_price', 'spread', 'vamp', 'vamp_mid_diff', 'trade_imbalance', 'quote_imbalance']].head())\n",
    "    \n",
    "    # Drop intermediate LOB level columns if they are no longer needed for modeling to save memory\n",
    "    cols_to_drop_intermediate = []\n",
    "    for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "        cols_to_drop_intermediate.extend([f'bid_price_L{i}', f'bid_qty_L{i}', f'ask_price_L{i}', f'ask_qty_L{i}'])\n",
    "    cols_to_drop_intermediate.extend(['Pb_vamp', 'Pa_vamp']) # VAMP intermediate calcs\n",
    "    # df_combined.drop(columns=[col for col in cols_to_drop_intermediate if col in df_combined.columns], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Target Variable Creation\n",
    "We want to predict future price changes. The PDF uses look-ahead windows from 1s to 60s.\n",
    "Let's use `PREDICTION_HORIZON_SECONDS`.\n",
    "Target: $MidPrice_{t+\\Delta t} - MidPrice_t$ (absolute change) or % change.\n",
    "Or, for classification: sign of change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mid_price' not in df_combined.columns or df_combined.empty:\n",
    "    print(\"Skipping target variable creation as mid_price is missing.\")\n",
    "else:\n",
    "    df_combined['future_mid_price'] = df_combined['mid_price'].shift(-PREDICTION_HORIZON_SECONDS)\n",
    "    \n",
    "    if TARGET_TYPE == 'regression':\n",
    "        # Predicting the actual price change\n",
    "        df_combined['target_price_change'] = df_combined['future_mid_price'] - df_combined['mid_price']\n",
    "        # Or percentage change:\n",
    "        # df_combined['target_price_change_pct'] = (df_combined['future_mid_price'] - df_combined['mid_price']) / df_combined['mid_price'] * 100\n",
    "        TARGET_COLUMN = 'target_price_change'\n",
    "    \n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        # Predicting price movement direction (-1 for down, 0 for neutral, 1 for up)\n",
    "        # The PDF uses strict inequalities for binary classification\n",
    "        # And thresholds (sigma) for multiclass\n",
    "        price_diff = df_combined['future_mid_price'] - df_combined['mid_price']\n",
    "        \n",
    "        # Simple binary: up (1) or down (-1), excluding no change\n",
    "        # df_combined['target_direction'] = np.sign(price_diff)\n",
    "        # df_combined = df_combined[df_combined['target_direction'] != 0] # Optional: remove neutral cases\n",
    "\n",
    "        # Tri-class: up (2), neutral (1), down (0) (common for classification)\n",
    "        # Define a threshold for \"neutral\" e.g., based on spread or a small fixed value\n",
    "        neutral_threshold = df_combined['spread'].mean() * 0.1 # Example threshold\n",
    "        df_combined['target_direction'] = 1 # Neutral\n",
    "        df_combined.loc[price_diff > neutral_threshold, 'target_direction'] = 2 # Up\n",
    "        df_combined.loc[price_diff < -neutral_threshold, 'target_direction'] = 0 # Down\n",
    "        TARGET_COLUMN = 'target_direction'\n",
    "\n",
    "    df_final = df_combined.dropna(subset=[TARGET_COLUMN]) # Remove rows where future price is NaN\n",
    "    \n",
    "    if not df_final.empty:\n",
    "        print(f\"Final data with target variable '{TARGET_COLUMN}' (head):\\n\", df_final[['mid_price', 'future_mid_price', TARGET_COLUMN] + [col for col in ['vamp_mid_diff', 'trade_imbalance', 'quote_imbalance'] if col in df_final.columns]].head())\n",
    "        print(f\"\\nTarget variable ({TARGET_COLUMN}) distribution:\")\n",
    "        if TARGET_TYPE == 'regression':\n",
    "            df_final[TARGET_COLUMN].plot(kind='hist', bins=50, title='Target Price Change Distribution')\n",
    "            plt.show()\n",
    "            print(df_final[TARGET_COLUMN].describe())\n",
    "        elif TARGET_TYPE == 'classification':\n",
    "            print(df_final[TARGET_COLUMN].value_counts(normalize=True))\n",
    "    else:\n",
    "        print(\"DataFrame is empty after target creation and NaN removal. Check data or prediction horizon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Stationarity Checks and Transformations\n",
    "Time series data for financial modeling often requires features to be stationary.\n",
    "The target variable (price change or returns) is usually stationary.\n",
    "We should check engineered features like `vamp_mid_diff`, `trade_imbalance`, `quote_imbalance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(series, series_name=''):\n",
    "    \"\"\"Performs ADF test and prints results.\"\"\"\n",
    "    if series.empty or series.isnull().all():\n",
    "        print(f\"Series {series_name} is empty or all NaN. Skipping stationarity check.\")\n",
    "        return\n",
    "    print(f'\\nStationarity Test for {series_name}:')\n",
    "    result = adfuller(series.dropna()) # Drop NaNs for test\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"Result: Likely Stationary (p-value <= 0.05) for {series_name}\")\n",
    "    else:\n",
    "        print(f\"Result: Likely Non-Stationary (p-value > 0.05) for {series_name}\")\n",
    "\n",
    "feature_columns_for_model = []\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    # Features to check and potentially use\n",
    "    potential_features = ['spread', 'vamp_mid_diff', 'trade_imbalance', 'quote_imbalance']\n",
    "    # Add other features like lagged mid_price changes, volatility, etc. if desired\n",
    "\n",
    "    for col in potential_features:\n",
    "        if col in df_final.columns:\n",
    "            check_stationarity(df_final[col], col)\n",
    "            # If a feature is found non-stationary, consider differencing:\n",
    "            # df_final[f'{col}_diff'] = df_final[col].diff()\n",
    "            # Then check stationarity of df_final[f'{col}_diff'] and use it in the model.\n",
    "            # For simplicity, we'll use original features here but this step is crucial.\n",
    "            feature_columns_for_model.append(col)\n",
    "    \n",
    "    if TARGET_TYPE == 'regression': # Target itself should be stationary if it's a difference\n",
    "        check_stationarity(df_final[TARGET_COLUMN], TARGET_COLUMN)\n",
    "    \n",
    "    # Ensure no NaN in features or target chosen for modeling after any transformations\n",
    "    df_model_ready = df_final[feature_columns_for_model + [TARGET_COLUMN]].copy()\n",
    "    df_model_ready.dropna(inplace=True)\n",
    "\n",
    "    if df_model_ready.empty:\n",
    "        print(\"DataFrame became empty after dropping NaNs for model features. Check feature engineering and stationarity steps.\")\n",
    "    else:\n",
    "        print(f\"\\nFeatures selected for modeling: {feature_columns_for_model}\")\n",
    "else:\n",
    "    print(\"Skipping stationarity checks as df_final is not available or empty.\")\n",
    "    df_model_ready = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_model_ready' not in locals() or df_model_ready.empty:\n",
    "    print(\"Skipping model building as data is not ready.\")\n",
    "else:\n",
    "    X = df_model_ready[feature_columns_for_model]\n",
    "    y = df_model_ready[TARGET_COLUMN]\n",
    "\n",
    "    # --- Data Splitting (Time Series Aware) ---\n",
    "    # Using TimeSeriesSplit for cross-validation would be more robust.\n",
    "    # For a simple train/test split:\n",
    "    train_size_pct = 0.8\n",
    "    split_idx = int(len(X) * train_size_pct)\n",
    "    \n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "    # --- Feature Scaling ---\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Store results\n",
    "    model_results = {}\n",
    "\n",
    "    def evaluate_model(name, model, X_test_data, y_true, y_pred):\n",
    "        if TARGET_TYPE == 'regression':\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            print(f\"{name} - MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "            model_results[name] = {'MSE': mse, 'R2': r2}\n",
    "            \n",
    "            # Plot predictions vs actuals for regression\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_true, y_pred, alpha=0.5, label='Predicted vs Actual')\n",
    "            plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2, label='Perfect Prediction')\n",
    "            plt.xlabel(\"Actual Values\")\n",
    "            plt.ylabel(\"Predicted Values\")\n",
    "            plt.title(f\"{name} - Predictions vs Actuals\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        elif TARGET_TYPE == 'classification':\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            model_results[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}\n",
    "\n",
    "    # --- Model Implementations ---\n",
    "\n",
    "    # 1. Linear Regression (for regression target) / Logistic Regression (for classification)\n",
    "    if TARGET_TYPE == 'regression':\n",
    "        print(\"\\n--- Linear Regression ---\")\n",
    "        model_lr = LinearRegression()\n",
    "        model_lr.fit(X_train_scaled, y_train)\n",
    "        y_pred_lr = model_lr.predict(X_test_scaled)\n",
    "        evaluate_model(\"Linear Regression\", model_lr, X_test_scaled, y_test, y_pred_lr)\n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        print(\"\\n--- Logistic Regression ---\")\n",
    "        model_logr = LogisticRegression(solver='liblinear', multi_class='auto', random_state=42, max_iter=1000)\n",
    "        model_logr.fit(X_train_scaled, y_train)\n",
    "        y_pred_logr = model_logr.predict(X_test_scaled)\n",
    "        evaluate_model(\"Logistic Regression\", model_logr, X_test_scaled, y_test, y_pred_logr)\n",
    "\n",
    "    # 2. Decision Tree\n",
    "    print(\"\\n--- Decision Tree ---\")\n",
    "    if TARGET_TYPE == 'regression':\n",
    "        model_dt = DecisionTreeRegressor(random_state=42, max_depth=10, min_samples_split=10)\n",
    "        model_dt.fit(X_train_scaled, y_train) # Can use unscaled X_train for trees too\n",
    "        y_pred_dt = model_dt.predict(X_test_scaled)\n",
    "        evaluate_model(\"Decision Tree Regressor\", model_dt, X_test_scaled, y_test, y_pred_dt)\n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        model_dtc = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=10)\n",
    "        model_dtc.fit(X_train_scaled, y_train)\n",
    "        y_pred_dtc = model_dtc.predict(X_test_scaled)\n",
    "        evaluate_model(\"Decision Tree Classifier\", model_dtc, X_test_scaled, y_test, y_pred_dtc)\n",
    "\n",
    "    # 3. Random Forest\n",
    "    print(\"\\n--- Random Forest ---\")\n",
    "    if TARGET_TYPE == 'regression':\n",
    "        model_rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10, n_jobs=-1)\n",
    "        model_rf.fit(X_train_scaled, y_train)\n",
    "        y_pred_rf = model_rf.predict(X_test_scaled)\n",
    "        evaluate_model(\"Random Forest Regressor\", model_rf, X_test_scaled, y_test, y_pred_rf)\n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        model_rfc = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10, n_jobs=-1)\n",
    "        model_rfc.fit(X_train_scaled, y_train)\n",
    "        y_pred_rfc = model_rfc.predict(X_test_scaled)\n",
    "        evaluate_model(\"Random Forest Classifier\", model_rfc, X_test_scaled, y_test, y_pred_rfc)\n",
    "\n",
    "    # 4. XGBoost\n",
    "    print(\"\\n--- XGBoost ---\")\n",
    "    if TARGET_TYPE == 'regression':\n",
    "        model_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42, max_depth=7, learning_rate=0.1, n_jobs=-1)\n",
    "        model_xgb.fit(X_train_scaled, y_train)\n",
    "        y_pred_xgb = model_xgb.predict(X_test_scaled)\n",
    "        evaluate_model(\"XGBoost Regressor\", model_xgb, X_test_scaled, y_test, y_pred_xgb)\n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        # Determine num_class for XGBClassifier if target is multi-class\n",
    "        num_class = len(np.unique(y_train)) if len(np.unique(y_train)) > 2 else 1 # Binary if 1 or 2 unique values (0,1)\n",
    "        objective_xgb_clf = 'multi:softmax' if num_class > 1 else 'binary:logistic' # num_class for multi:softmax\n",
    "        \n",
    "        model_xgbc_params = {\n",
    "            'n_estimators': 100, 'random_state': 42, 'max_depth': 7, 'learning_rate': 0.1, 'n_jobs': -1\n",
    "        }\n",
    "        if objective_xgb_clf == 'multi:softmax':\n",
    "             model_xgbc_params['objective'] = 'multi:softmax'\n",
    "             model_xgbc_params['num_class'] = num_class\n",
    "        else:\n",
    "            model_xgbc_params['objective'] = 'binary:logistic'\n",
    "            \n",
    "        model_xgbc = xgb.XGBClassifier(**model_xgbc_params)\n",
    "        model_xgbc.fit(X_train_scaled, y_train)\n",
    "        y_pred_xgbc = model_xgbc.predict(X_test_scaled)\n",
    "        evaluate_model(\"XGBoost Classifier\", model_xgbc, X_test_scaled, y_test, y_pred_xgbc)\n",
    "        \n",
    "    # 5. Support Vector Machine (SVM)\n",
    "    # SVM can be slow on large datasets. Consider using a subset of data if training takes too long.\n",
    "    print(\"\\n--- Support Vector Machine (SVM) ---\")\n",
    "    if TARGET_TYPE == 'regression':\n",
    "        model_svr = SVR(kernel='rbf', C=1.0, epsilon=0.1) # Parameters can be tuned\n",
    "        model_svr.fit(X_train_scaled, y_train)\n",
    "        y_pred_svr = model_svr.predict(X_test_scaled)\n",
    "        evaluate_model(\"SVR\", model_svr, X_test_scaled, y_test, y_pred_svr)\n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        model_svc = SVC(kernel='rbf', C=1.0, random_state=42, probability=True) # probability=True for predict_proba if needed\n",
    "        model_svc.fit(X_train_scaled, y_train)\n",
    "        y_pred_svc = model_svc.predict(X_test_scaled)\n",
    "        evaluate_model(\"SVC\", model_svc, X_test_scaled, y_test, y_pred_svc)\n",
    "\n",
    "    # 6. Neural Network (MLP)\n",
    "    print(\"\\n--- Neural Network (MLP) ---\")\n",
    "    def create_mlp(input_dim, num_classes=1, classification=False):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        if classification:\n",
    "            if num_classes == 1 or num_classes == 2: # Binary classification\n",
    "                 model.add(Dense(1, activation='sigmoid'))\n",
    "                 model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "            else: # Multiclass classification\n",
    "                 model.add(Dense(num_classes, activation='softmax'))\n",
    "                 model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # use sparse if y is integer encoded\n",
    "        else: # Regression\n",
    "            model.add(Dense(1, activation='linear')) # Linear activation for regression\n",
    "            model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    if TARGET_TYPE == 'regression':\n",
    "        model_mlp = create_mlp(X_train_scaled.shape[1], classification=False)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        model_mlp.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stop], verbose=0)\n",
    "        y_pred_mlp = model_mlp.predict(X_test_scaled).flatten()\n",
    "        evaluate_model(\"MLP Regressor\", model_mlp, X_test_scaled, y_test, y_pred_mlp)\n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        num_classes_nn = len(np.unique(y_train))\n",
    "        model_mlpc = create_mlp(X_train_scaled.shape[1], num_classes=num_classes_nn, classification=True)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        # Ensure y_train is appropriate for the loss function (e.g. 0,1 for binary_crossentropy; 0,1,2 for sparse_categorical)\n",
    "        model_mlpc.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stop], verbose=0)\n",
    "        \n",
    "        if num_classes_nn <= 2: # Binary\n",
    "            y_pred_mlpc_proba = model_mlpc.predict(X_test_scaled)\n",
    "            y_pred_mlpc = (y_pred_mlpc_proba > 0.5).astype(int).flatten()\n",
    "        else: # Multiclass\n",
    "            y_pred_mlpc_proba = model_mlpc.predict(X_test_scaled)\n",
    "            y_pred_mlpc = np.argmax(y_pred_mlpc_proba, axis=1)\n",
    "            \n",
    "        evaluate_model(\"MLP Classifier\", model_mlpc, X_test_scaled, y_test, y_pred_mlpc)\n",
    "\n",
    "    # --- Display Final Results ---\n",
    "    print(\"\\n--- Model Performance Summary ---\")\n",
    "    results_df = pd.DataFrame(model_results).T\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Conclusion and Future Work\n",
    "\n",
    "This notebook provided a framework for acquiring, preprocessing, and modeling cryptocurrency order book data for short-term price prediction. Key features inspired by \"Mind the Gaps\" such as Mid-Price, Spread, VAMP, Trade Imbalance, and Quote Imbalance were implemented.\n",
    "\n",
    "**Observations from this run (based on sample data/placeholder logic):**\n",
    "* (Actual observations will depend on the real data and model performance)\n",
    "* The VAMP feature was noted in the paper as a strong predictor. Its effectiveness would depend on the quality and depth of LOB data used.\n",
    "* Trade Imbalance and Quote Imbalance aim to capture market pressure.\n",
    "\n",
    "**Future Work:**\n",
    "* **Robust Data Pipeline**: Implement a more robust data acquisition pipeline, especially for live API data (e.g., using WebSockets for continuous LOB updates and reconstruction).\n",
    "* **Advanced Feature Engineering**:\n",
    "    * Explore more sophisticated weighting for Trade Imbalance.\n",
    "    * Test different VAMP liquidity cutoffs and QI levels systematically.\n",
    "    * Incorporate features like realized volatility, order flow toxicity, or market impact models.\n",
    "* **Hyperparameter Tuning**: Systematically tune hyperparameters for each ML model (e.g., using GridSearchCV or RandomizedSearchCV with TimeSeriesSplit).\n",
    "* **Stationarity**: Rigorously ensure all features used in models are stationary. Apply transformations like differencing if needed and re-evaluate.\n",
    "* **Model Ensembling/Stacking**: Combine predictions from multiple models to potentially improve performance.\n",
    "* **Deeper Neural Networks**: Explore more complex architectures like LSTMs or GRUs, which are well-suited for time series data.\n",
    "* **Alternative Prediction Targets**: Expand on the binary and multiclass classification approaches from the paper, especially predicting one-standard-deviation price movements.\n",
    "* **Backtesting Framework**: Develop a rigorous backtesting framework that accounts for transaction costs, slippage, and realistic trading conditions. The P&L metric in the paper is a good starting point.\n",
    "* **Expand Dataset**: Analyze data across different crypto assets and exchanges, and longer time periods, including diverse market conditions (e.g., high volatility periods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### References from \"Mind the Gaps\" used in this notebook:\n",
    "- [1] Martin, P., Line Jr., W., Feng, Y., Yang, Y., Zheng, S., Qi, S., & Zhu, B. (2022). *Mind the Gaps: Short-term Crypto Price Prediction*. Cornell University. Available at SSRN: https://ssrn.com/abstract=4351947\n",
    "- [16] Prediction at time scales from one second to 60 seconds.\n",
    "- [18] Volume-Adjusted Mid-Price as the ultimate short-term predictor.\n",
    "- [19, 20] Data sourcing: Bitstamp, three full months, tick level.\n",
    "- [21] Initial feature calculation: spread, mid-price, best bid/ask, volume-adjusted versions.\n",
    "- [25, 26] Condensing dataset to full seconds.\n",
    "- [48, 49] Volume-Adjusted Mid-Price (VAMP) definition and formula.\n",
    "- [52] Plotting (mid-price - VAMP) against returns.\n",
    "- [54, 55, 159] VAMP volume cutoffs, settling on $50k-$60k range, specifically $60k.\n",
    "- [56, 57, 58] Trade Imbalance (TI) definition, formula with linear weight, range -1 to 1.\n",
    "- [72] Using 1-minute window for Trade Imbalance.\n",
    "- [88, 89, 94] Quote Imbalance (QI) definition, formula, range -1 to 1, using up to level 5.\n",
    "- [93] QI relationship becoming more linear with deeper levels.\n",
    "- [128] Trading P&L metric introduction.\n",
    "- [152] Binary classification setup: strict inequalities for price change prediction.\n",
    "- [171] Multiclass classification setup: one standard deviation thresholds.\n",
    "- [195] Expanding data to include diverse BTC data and volatile conditions.\n",
    "\n",
    "Note: Citation numbers in the markdown cells (e.g., `[cite: X]`) refer to page numbers or specific findings in the provided PDF \"Mind-the-Gaps-Short-term-Crypto-Price-Prediction-2022.pdf\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
