{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e088512f",
   "metadata": {},
   "source": [
    "# Short-term Crypto Price Prediction based on Order Book Dynamics\n",
    "\n",
    "This notebook aims to implement and explore methods for short-term cryptocurrency price prediction, drawing inspiration from the research paper \"Mind the Gaps: Short-term Crypto Price Prediction\".\n",
    "\n",
    "## Objective\n",
    "Conduct quantitative research on order book dynamics and build predictive models to forecast future prices.\n",
    "\n",
    "## Scope of Work\n",
    "1. Download/Fetch order book data.\n",
    "2. Engineer features from the order book.\n",
    "3. Create ML/statistical models to predict future price changes.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Configuration\n",
    "2. Data Acquisition\n",
    "   - Method 1: Fetching from Bybit API (using `pybit` REST & WebSocket)\n",
    "   - Method 2: Loading from a local dataset\n",
    "3. Data Preprocessing\n",
    "   - LOB Reconstruction (if necessary)\n",
    "   - Resampling to Second-Level Data\n",
    "   - Cleaning\n",
    "4. Feature Engineering\n",
    "   - Mid-Price and Spread\n",
    "   - Volume-Adjusted Mid-Price (VAMP)\n",
    "   - Trade Imbalance (TI)\n",
    "   - Quote Imbalance (QI)\n",
    "5. Target Variable Creation\n",
    "6. Stationarity Checks and Transformations\n",
    "7. Model Building and Evaluation\n",
    "   - Data Splitting\n",
    "   - Linear Regression\n",
    "   - Logistic Regression (for classification)\n",
    "   - Decision Tree\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - Support Vector Machine (SVM)\n",
    "   - Neural Network (MLP)\n",
    "8. Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6399adb",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Configuration\n",
    "Import necessary libraries and configure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1756d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pybit.unified_trading import HTTP as UnifiedHTTP # For REST API\n",
    "from pybit.unified_trading import WebSocket as UnifiedWebSocket # For WebSocket\n",
    "import time\n",
    "import datetime\n",
    "import logging # For pybit websocket\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Setup logging for pybit (optional, but helpful for WebSocket debugging)\n",
    "logger = logging.getLogger(\"pybit\")\n",
    "logger.setLevel(logging.INFO) # You can set to DEBUG for more verbose output\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n",
    "if not logger.hasHandlers(): # Avoid adding multiple handlers if re-running cell\n",
    "    logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61493b",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "Set your API keys for Bybit here if you choose Method 1.\n",
    "Set the path to your local dataset if you choose Method 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f09101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATA_SOURCE_METHOD = 'local_dataset' # Options: 'bybit_api', 'bybit_api_websocket_demo', 'local_dataset'\n",
    "\n",
    "# For Bybit API (Method 1)\n",
    "BYBIT_API_KEY = 'YOUR_API_KEY' # For private endpoints if needed, not required for public data\n",
    "BYBIT_SECRET_KEY = 'YOUR_SECRET_KEY' # For private endpoints\n",
    "BYBIT_SYMBOL = 'BTCUSDT' # Pybit uses symbols like BTCUSDT (no slash for USDT perpetual)\n",
    "BYBIT_LOB_DEPTH_SNAPSHOT = 50 # For REST snapshot (max 50 for unified trading orderbook endpoint)\n",
    "BYBIT_LOB_DEPTH_WEBSOCKET = 50 # For WebSocket stream (e.g., 1, 50, 200, 500)\n",
    "BYBIT_TRADE_LIMIT = 50 # Pybit public_trading_records limit is max 1000, default 500 for REST\n",
    "BYBIT_CHANNEL_TYPE = \"linear\" # For USDT perpetuals like BTCUSDT. Other options: \"spot\", \"inverse\", \"option\"\n",
    "\n",
    "# For Local Dataset (Method 2)\n",
    "LOCAL_ORDERBOOK_FILEPATH = 'path/to/your/orderbook_data.csv'\n",
    "LOCAL_TRADES_FILEPATH = 'path/to/your/trades_data.csv'\n",
    "\n",
    "# Feature Engineering & Modeling Parameters\n",
    "VAMP_LIQUIDITY_CUTOFF = 60000 # In dollars, as per PDF findings\n",
    "TRADE_IMBALANCE_WINDOW = '60S' # 60 seconds for TI calculation\n",
    "QUOTE_IMBALANCE_LEVELS = 5 # Number of LOB levels for QI (ensure data has this depth)\n",
    "PREDICTION_HORIZON_SECONDS = 60 # Predict price change 60 seconds ahead\n",
    "TARGET_TYPE = 'regression' # 'regression' or 'classification' (for direction: up/down/neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe762b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13655e4",
   "metadata": {},
   "source": [
    "### Method 1: Fetching from Bybit API (using `pybit` REST client for snapshots)\n",
    "This section contains functions to fetch Limit Order Book (LOB) snapshots and trade data from Bybit using `pybit`'s REST client. A separate section below discusses real-time data with WebSockets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bybit_order_book_snapshot_pybit(symbol, depth_limit):\n",
    "    \"\"\"Fetches the current order book snapshot from Bybit using pybit's Unified Trading REST API.\"\"\"\n",
    "    session = UnifiedHTTP(\n",
    "        testnet=False, # Set to True for testnet\n",
    "        # api_key=BYBIT_API_KEY, # Not needed for public order book\n",
    "        # api_secret=BYBIT_SECRET_KEY\n",
    "    )\n",
    "    try:\n",
    "        # For Unified Trading, category is 'linear', 'spot', 'inverse', 'option'\n",
    "        response = session.get_orderbook(category=BYBIT_CHANNEL_TYPE, symbol=symbol, limit=depth_limit)\n",
    "        \n",
    "        if response and response.get('retCode') == 0:\n",
    "            order_book_result = response['result']\n",
    "            timestamp_ms = int(response['time']) # Timestamp of the response\n",
    "            \n",
    "            # Bids are sorted descending by price, Asks ascending by price in Bybit's response\n",
    "            bids_list = [[float(p), float(q)] for p, q in order_book_result['b']]\n",
    "            asks_list = [[float(p), float(q)] for p, q in order_book_result['a']]\n",
    "            \n",
    "            bids_df = pd.DataFrame(bids_list, columns=['price', 'qty'])\n",
    "            asks_df = pd.DataFrame(asks_list, columns=['price', 'qty'])\n",
    "            \n",
    "            # Add cumulative volume for VAMP calculation later (if needed from snapshot)\n",
    "            if not bids_df.empty:\n",
    "                bids_df['cumulative_qty'] = bids_df['qty'].cumsum()\n",
    "                bids_df['cumulative_value_usd'] = (bids_df['price'] * bids_df['qty']).cumsum()\n",
    "            if not asks_df.empty:\n",
    "                asks_df['cumulative_qty'] = asks_df['qty'].cumsum()\n",
    "                asks_df['cumulative_value_usd'] = (asks_df['price'] * asks_df['qty']).cumsum()\n",
    "                \n",
    "            return bids_df, asks_df, timestamp_ms\n",
    "        else:\n",
    "            print(f\"Error in Bybit API response for order book {symbol}: {response}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception fetching Bybit order book snapshot for {symbol} with pybit: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), None\n",
    "\n",
    "def fetch_bybit_trades_pybit(symbol, limit):\n",
    "    \"\"\"Fetches recent trades from Bybit using pybit's Unified Trading REST API.\"\"\"\n",
    "    session = UnifiedHTTP(\n",
    "        testnet=False, \n",
    "    )\n",
    "    try:\n",
    "        response = session.get_public_trade_history(category=BYBIT_CHANNEL_TYPE, symbol=symbol, limit=limit)\n",
    "        if response and response.get('retCode') == 0:\n",
    "            trades_data = response['result']['list']\n",
    "            if not trades_data: \n",
    "                 return pd.DataFrame()\n",
    "            df_trades = pd.DataFrame(trades_data)\n",
    "            df_trades['timestamp'] = pd.to_datetime(df_trades['T'], unit='ms') # 'T' is trade time in ms\n",
    "            df_trades.rename(columns={'s': 'symbol', 'S': 'side', 'v': 'qty', 'p': 'price'}, inplace=True)\n",
    "            df_trades['side'] = df_trades['side'].apply(lambda x: 'buy' if x == 'Buy' else 'sell') # Standardize side\n",
    "            df_trades['price'] = df_trades['price'].astype(float)\n",
    "            df_trades['qty'] = df_trades['qty'].astype(float)\n",
    "            df_trades['value_usd'] = df_trades['price'] * df_trades['qty']\n",
    "            df_trades = df_trades[['timestamp', 'price', 'qty', 'side', 'value_usd']]\n",
    "            return df_trades.sort_values('timestamp').reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"Error in Bybit API response for trades {symbol}: {response}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception fetching Bybit trades for {symbol} with pybit: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "if DATA_SOURCE_METHOD == 'bybit_api':\n",
    "    print(\"Attempting to fetch data from Bybit API using pybit (REST snapshot)...\")\n",
    "    bids_df_pybit, asks_df_pybit, lob_timestamp_pybit = fetch_bybit_order_book_snapshot_pybit(BYBIT_SYMBOL, BYBIT_LOB_DEPTH_SNAPSHOT)\n",
    "    trades_df_pybit = fetch_bybit_trades_pybit(BYBIT_SYMBOL, BYBIT_TRADE_LIMIT)\n",
    "\n",
    "    if not (bids_df_pybit.empty or asks_df_pybit.empty or lob_timestamp_pybit is None):\n",
    "        print(f\"\\nOrder Book Snapshot (pybit) at {datetime.datetime.fromtimestamp(lob_timestamp_pybit/1000)}:\")\n",
    "        print(\"Top 5 Bids:\\n\", bids_df_pybit.head())\n",
    "        print(\"Top 5 Asks:\\n\", asks_df_pybit.head())\n",
    "        # Prepare df_lob_raw from this single snapshot for the rest of the notebook\n",
    "        temp_lob_data = []\n",
    "        entry = {'timestamp': pd.to_datetime(lob_timestamp_pybit, unit='ms')}\n",
    "        entry['best_bid_price'] = bids_df_pybit['price'].iloc[0]\n",
    "        entry['best_bid_qty'] = bids_df_pybit['qty'].iloc[0]\n",
    "        entry['best_ask_price'] = asks_df_pybit['price'].iloc[0]\n",
    "        entry['best_ask_qty'] = asks_df_pybit['qty'].iloc[0]\n",
    "        for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "            if len(bids_df_pybit) >= i: \n",
    "                entry[f'bid_price_L{i}'] = bids_df_pybit['price'].iloc[i-1]\n",
    "                entry[f'bid_qty_L{i}'] = bids_df_pybit['qty'].iloc[i-1]\n",
    "            if len(asks_df_pybit) >= i:\n",
    "                entry[f'ask_price_L{i}'] = asks_df_pybit['price'].iloc[i-1]\n",
    "                entry[f'ask_qty_L{i}'] = asks_df_pybit['qty'].iloc[i-1]\n",
    "        temp_lob_data.append(entry)\n",
    "        df_lob_raw = pd.DataFrame(temp_lob_data).set_index('timestamp')\n",
    "    else:\n",
    "        print(\"Could not fetch order book snapshot using pybit or it was empty.\")\n",
    "        df_lob_raw = pd.DataFrame()\n",
    "        \n",
    "    if not trades_df_pybit.empty:\n",
    "        print(\"\\nRecent Trades (pybit):\\n\", trades_df_pybit.head())\n",
    "        df_trades_raw = trades_df_pybit\n",
    "    else:\n",
    "        print(\"Could not fetch trades using pybit or no trades found.\")\n",
    "        df_trades_raw = pd.DataFrame()\n",
    "    \n",
    "    print(\"\\nNote: Bybit API (REST) provides a single snapshot. For robust analysis, collect data over time or use WebSockets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862750a",
   "metadata": {},
   "source": [
    "### Method 1b: Real-time Order Book Data with `pybit.unified_trading.WebSocket` (Conceptual)\n",
    "\n",
    "Fetching true real-time data requires a WebSocket connection. The following cell provides a conceptual example using `pybit.unified_trading.WebSocket` for order book data. This data would typically be streamed to a database or file for later batch processing by the rest of this notebook, or used in a dedicated real-time prediction system.\n",
    "\n",
    "**This code is for demonstration and would run indefinitely to collect data. You would typically run it in a separate script or manage its lifecycle carefully within a notebook (e.g., run for a short period to collect sample data).**\n",
    "\n",
    "**Important Note on LOB Reconstruction:** The WebSocket stream provides initial `snapshot` messages followed by `delta` updates. To use this data for features like VAMP or QI at specific time intervals (e.g., every second), you need to:\n",
    "1.  Maintain a local, in-memory representation of the order book (e.g., sorted lists or DataFrames for bids and asks).\n",
    "2.  Initialize this local LOB with the first `snapshot` message.\n",
    "3.  For each incoming `delta` message, apply the changes (delete, update, insert) to your local LOB. Price levels with size \"0\" should be removed.\n",
    "4.  At your desired sampling frequency (e.g., every 1 second), take a snapshot of your reconstructed local LOB. This snapshot would then be processed to create rows similar to what `df_lob_raw` expects (e.g., best bid/ask, L2 prices/quantities, etc.).\n",
    "This reconstruction logic is non-trivial and is **not implemented** in the example below, which focuses on connection and basic message parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8853401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collected_ws_orderbook_data = [] # Global list to store raw messages for this demo\n",
    "MAX_WS_MESSAGES_TO_COLLECT = 10 # Collect a few messages then stop for demo purposes\n",
    "ws_client_global = None # To allow exiting from the handler\n",
    "\n",
    "def handle_unified_orderbook_message(message):\n",
    "    \"\"\"Handles incoming order book messages from Bybit Unified Trading WebSocket.\"\"\"\n",
    "    global collected_ws_orderbook_data, ws_client_global\n",
    "    # print(f\"Raw WS Message: {message}\") # For debugging\n",
    "    \n",
    "    msg_type = message.get('type')\n",
    "    topic = message.get('topic')\n",
    "    timestamp_ms = message.get('ts')\n",
    "    data = message.get('data')\n",
    "\n",
    "    if not data:\n",
    "        logger.warning(f\"Received message with no data: {message}\")\n",
    "        return\n",
    "\n",
    "    symbol = data.get('s')\n",
    "    bids_raw = data.get('b', []) # List of [price_str, size_str]\n",
    "    asks_raw = data.get('a', []) # List of [price_str, size_str]\n",
    "    update_id = data.get('u')\n",
    "    sequence_id = data.get('seq')\n",
    "    # cts = message.get('cts') # Cross timestamp\n",
    "\n",
    "    logger.info(f\"WS OrderBook: Type='{msg_type}', Symbol='{symbol}', Topic='{topic}', TS={timestamp_ms}, U_ID={update_id}, Seq={sequence_id}\")\n",
    "\n",
    "    # Basic parsing of bids/asks into lists of floats\n",
    "    bids_parsed = [[float(p), float(s)] for p, s in bids_raw]\n",
    "    asks_parsed = [[float(p), float(s)] for p, s in asks_raw]\n",
    "\n",
    "    collected_ws_orderbook_data.append({\n",
    "        'timestamp_ms': timestamp_ms,\n",
    "        'type': msg_type,\n",
    "        'symbol': symbol,\n",
    "        'bids': bids_parsed,\n",
    "        'asks': asks_parsed,\n",
    "        'update_id': update_id,\n",
    "        'sequence_id': sequence_id\n",
    "    })\n",
    "    \n",
    "    # Placeholder for LOB reconstruction logic:\n",
    "    if msg_type == 'snapshot':\n",
    "        # print(f\"Received SNAPSHOT for {symbol}. Bids: {len(bids_parsed)}, Asks: {len(asks_parsed)}. Initialize local LOB.\")\n",
    "        # local_orderbook.initialize(bids_parsed, asks_parsed)\n",
    "        pass\n",
    "    elif msg_type == 'delta':\n",
    "        # print(f\"Received DELTA for {symbol}. Bids: {len(bids_parsed)}, Asks: {len(asks_parsed)}. Update local LOB.\")\n",
    "        # local_orderbook.update(bids_parsed, asks_parsed) # bids_parsed/asks_parsed here are deltas\n",
    "        pass\n",
    "        \n",
    "    if len(collected_ws_orderbook_data) >= MAX_WS_MESSAGES_TO_COLLECT:\n",
    "        if ws_client_global and ws_client_global.is_connected():\n",
    "            logger.info(f\"Collected {MAX_WS_MESSAGES_TO_COLLECT} WebSocket messages. Attempting to stop client.\")\n",
    "            ws_client_global.exit()\n",
    "\n",
    "if DATA_SOURCE_METHOD == 'bybit_api_websocket_demo':\n",
    "    print(\"Starting Bybit Unified Trading WebSocket for real-time order book data (demo)...\\n\")\n",
    "    collected_ws_orderbook_data = [] # Reset for each run\n",
    "    \n",
    "    ws_client_global = UnifiedWebSocket(\n",
    "        testnet=False, # Set to True for Bybit testnet\n",
    "        channel_type=BYBIT_CHANNEL_TYPE \n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Subscribing to orderbook.{BYBIT_LOB_DEPTH_WEBSOCKET}.{BYBIT_SYMBOL}\")\n",
    "    ws_client_global.orderbook_stream(\n",
    "        depth=BYBIT_LOB_DEPTH_WEBSOCKET, \n",
    "        symbol=BYBIT_SYMBOL, \n",
    "        callback=handle_unified_orderbook_message\n",
    "    )\n",
    "    \n",
    "    print(f\"Collecting up to {MAX_WS_MESSAGES_TO_COLLECT} messages. This might take a few seconds depending on market activity...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        while ws_client_global.is_connected() and len(collected_ws_orderbook_data) < MAX_WS_MESSAGES_TO_COLLECT:\n",
    "            time.sleep(0.1) # Check frequently\n",
    "            if time.time() - start_time > 60: # Timeout after 60 seconds if not enough messages\n",
    "                logger.warning(\"WebSocket demo timeout after 60 seconds.\")\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"WebSocket interrupted by user.\")\n",
    "    finally:\n",
    "        if ws_client_global and ws_client_global.is_connected():\n",
    "            ws_client_global.exit()\n",
    "        logger.info(\"WebSocket connection closed.\")\n",
    "        \n",
    "    if collected_ws_orderbook_data:\n",
    "        print(f\"\\n--- Collected {len(collected_ws_orderbook_data)} WebSocket Order Book Messages (Raw Sample) ---\")\n",
    "        for i, msg_data in enumerate(collected_ws_orderbook_data[:min(3, len(collected_ws_orderbook_data))]): \n",
    "            print(f\"Message {i+1}: TS={msg_data['timestamp_ms']}, Type={msg_data['type']}, Symbol={msg_data['symbol']}\")\n",
    "            print(f\"  Bids sample: {msg_data.get('bids')[:2]}\")\n",
    "            print(f\"  Asks sample: {msg_data.get('asks')[:2]}\")\n",
    "        print(\"\\nReminder: Full LOB reconstruction from snapshot & deltas is needed to use this data for batch analysis.\")\n",
    "        # Example: df_from_ws = pd.DataFrame(collected_ws_orderbook_data)\n",
    "        # This df_from_ws would then need extensive processing to create time-series LOB snapshots.\n",
    "elif DATA_SOURCE_METHOD == 'bybit_api':\n",
    "    pass # Data already handled by REST snapshot logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103003fb",
   "metadata": {},
   "source": [
    "### Method 2: Loading from a Local Dataset\n",
    "Load data from a CSV file. The PDF uses three months of tick-level data from Bitstamp.\n",
    "You'll need to adapt the loading based on your dataset's format.\n",
    "\n",
    "For this example, we'll assume two files:\n",
    "1.  `orderbook_data.csv`: Contains time-series of LOB snapshots (e.g., best bid/ask, and deeper levels if available).\n",
    "    Columns might be: `timestamp`, `best_bid_price`, `best_bid_qty`, `best_ask_price`, `best_ask_qty`, `bid_price_L2`, `bid_qty_L2`, ... `ask_price_L5`, `ask_qty_L5`.\n",
    "2.  `trades_data.csv`: Contains historical trades.\n",
    "    Columns might be: `timestamp`, `price`, `qty`, `side` (e.g., 'buy' or 'sell')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SOURCE_METHOD == 'local_dataset':\n",
    "    print(f\"Loading data from local files: {LOCAL_ORDERBOOK_FILEPATH} and {LOCAL_TRADES_FILEPATH}\")\n",
    "    try:\n",
    "        # --- Load Order Book Data ---\n",
    "        try:\n",
    "            df_lob_raw = pd.read_csv(LOCAL_ORDERBOOK_FILEPATH)\n",
    "            df_lob_raw['timestamp'] = pd.to_datetime(df_lob_raw['timestamp'])\n",
    "            df_lob_raw.set_index('timestamp', inplace=True)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {LOCAL_ORDERBOOK_FILEPATH} not found. Generating sample LOB data.\")\n",
    "            sample_timestamps_lob = pd.to_datetime(pd.date_range(start='2023-01-01', periods=10000, freq='100ms').values)\n",
    "            data_lob = {\n",
    "                'timestamp': sample_timestamps_lob,\n",
    "                'best_bid_price': np.random.uniform(20000, 20100, 10000),\n",
    "                'best_bid_qty': np.random.uniform(0.1, 5, 10000),\n",
    "                'best_ask_price': np.random.uniform(20101, 20200, 10000),\n",
    "                'best_ask_qty': np.random.uniform(0.1, 5, 10000),\n",
    "            }\n",
    "            for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "                data_lob[f'bid_price_L{i}'] = data_lob['best_bid_price'] - i * np.random.uniform(0.5, 2, 10000)\n",
    "                data_lob[f'bid_qty_L{i}'] = np.random.uniform(0.1, 3, 10000)\n",
    "                data_lob[f'ask_price_L{i}'] = data_lob['best_ask_price'] + i * np.random.uniform(0.5, 2, 10000)\n",
    "                data_lob[f'ask_qty_L{i}'] = np.random.uniform(0.1, 3, 10000)\n",
    "            data_lob['best_ask_price'] = np.maximum(data_lob['best_ask_price'], data_lob['best_bid_price'] + 0.01)\n",
    "            for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "                 data_lob[f'ask_price_L{i}'] = np.maximum(data_lob[f'ask_price_L{i}'], data_lob[f'bid_price_L{i}'] + (i*0.1))\n",
    "            df_lob_raw = pd.DataFrame(data_lob).set_index('timestamp')\n",
    "\n",
    "        # --- Load Trades Data ---\n",
    "        try:\n",
    "            df_trades_raw = pd.read_csv(LOCAL_TRADES_FILEPATH)\n",
    "            df_trades_raw['timestamp'] = pd.to_datetime(df_trades_raw['timestamp'])\n",
    "            if 'value_usd' not in df_trades_raw.columns and 'price' in df_trades_raw.columns and 'qty' in df_trades_raw.columns:\n",
    "                 df_trades_raw['value_usd'] = df_trades_raw['price'].astype(float) * df_trades_raw['qty'].astype(float)\n",
    "            df_trades_raw.set_index('timestamp', inplace=True)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {LOCAL_TRADES_FILEPATH} not found. Generating sample trades data.\")\n",
    "            sample_timestamps_trades = df_lob_raw.index if not df_lob_raw.empty else pd.to_datetime(pd.date_range(start='2023-01-01', periods=10000, freq='100ms').values)\n",
    "            base_price = (df_lob_raw['best_bid_price'] + df_lob_raw['best_ask_price']) / 2 if not df_lob_raw.empty and 'best_bid_price' in df_lob_raw else pd.Series(np.random.uniform(20000,20200,len(sample_timestamps_trades)), index=sample_timestamps_trades)\n",
    "            data_trades = {\n",
    "                'timestamp': sample_timestamps_trades,\n",
    "                'price': base_price,\n",
    "                'qty': np.random.uniform(0.01, 1, len(sample_timestamps_trades)),\n",
    "                'side': np.random.choice(['buy', 'sell'], len(sample_timestamps_trades)),\n",
    "            }\n",
    "            df_trades_raw = pd.DataFrame(data_trades).set_index('timestamp')\n",
    "            df_trades_raw['value_usd'] = df_trades_raw['price'] * df_trades_raw['qty']\n",
    "        \n",
    "        print(\"LOB data (raw head):\\n\", df_lob_raw.head())\n",
    "        print(\"Trades data (raw head):\\n\", df_trades_raw.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading local data: {e}. Falling back to empty DataFrames.\")\n",
    "        df_lob_raw = pd.DataFrame()\n",
    "        df_trades_raw = pd.DataFrame()\n",
    "elif DATA_SOURCE_METHOD == 'bybit_api':\n",
    "    if 'df_lob_raw' not in locals(): df_lob_raw = pd.DataFrame()\n",
    "    if 'df_trades_raw' not in locals(): df_trades_raw = pd.DataFrame()\n",
    "    print(\"Using data fetched from Bybit API (REST snapshot).\")\n",
    "    print(\"LOB data (raw head from API snapshot):\\n\", df_lob_raw.head())\n",
    "    print(\"Trades data (raw head from API):\\n\", df_trades_raw.head())\n",
    "elif DATA_SOURCE_METHOD == 'bybit_api_websocket_demo':\n",
    "    print(\"WebSocket demo ran. `collected_ws_orderbook_data` contains raw messages.\")\n",
    "    print(\"Further processing is needed to use this data for batch analysis.\")\n",
    "    # For the rest of the notebook to run with some data, we might generate sample data here\n",
    "    # or the user should switch DATA_SOURCE_METHOD to 'local_dataset' or 'bybit_api' (REST)\n",
    "    if 'df_lob_raw' not in locals() or df_lob_raw.empty:\n",
    "        print(\"Generating sample LOB data as WebSocket demo does not populate df_lob_raw directly.\")\n",
    "        sample_timestamps_lob = pd.to_datetime(pd.date_range(start='2023-01-01', periods=1000, freq='1s').values)\n",
    "        data_lob = {'timestamp': sample_timestamps_lob,'best_bid_price': np.random.uniform(20000, 20100, 1000),'best_bid_qty': np.random.uniform(0.1, 5, 1000),'best_ask_price': np.random.uniform(20101, 20200, 1000),'best_ask_qty': np.random.uniform(0.1, 5, 1000)}\n",
    "        for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "            data_lob[f'bid_price_L{i}'] = data_lob['best_bid_price'] - i * np.random.uniform(0.5,2,1000); data_lob[f'bid_qty_L{i}'] = np.random.uniform(0.1,3,1000)\n",
    "            data_lob[f'ask_price_L{i}'] = data_lob['best_ask_price'] + i * np.random.uniform(0.5,2,1000); data_lob[f'ask_qty_L{i}'] = np.random.uniform(0.1,3,1000)\n",
    "        df_lob_raw = pd.DataFrame(data_lob).set_index('timestamp')\n",
    "    if 'df_trades_raw' not in locals() or df_trades_raw.empty:\n",
    "        print(\"Generating sample trades data as WebSocket demo does not populate df_trades_raw directly.\")\n",
    "        df_trades_raw = pd.DataFrame({'timestamp': df_lob_raw.index, 'price': (df_lob_raw.best_bid_price+df_lob_raw.best_ask_price)/2, 'qty': np.random.uniform(0.01,1,len(df_lob_raw)), 'side':np.random.choice(['buy','sell'],len(df_lob_raw))}).set_index('timestamp')\n",
    "        df_trades_raw['value_usd'] = df_trades_raw.price * df_trades_raw.qty\n",
    "else:\n",
    "    print(\"Invalid DATA_SOURCE_METHOD selected or API data fetch failed.\")\n",
    "    df_lob_raw = pd.DataFrame()\n",
    "    df_trades_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677e1c5",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preprocessing\n",
    "The PDF mentions condensing the full dataset to consider full seconds rather than every tick update to reduce data size.\n",
    "We will resample the data to 1-second intervals.\n",
    "\n",
    "**Note on LOB Reconstruction for VAMP/QI from Tick Data:**\n",
    "If `df_lob_raw` is from tick-by-tick updates (not snapshots), you'd need a more sophisticated LOB reconstruction process before this step. You would maintain the state of the order book at each tick and then sample it every second. The PDF mentions: \"Starting with the end of day snapshot, we then used each quote update to first update the order book, and then calculated and stored the important features\".\n",
    "For simplicity, if using `local_dataset`, we assume `df_lob_raw` provides snapshots or already has necessary levels available at each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_lob_raw.empty:\n",
    "    # Resample LOB data to 1-second frequency. Use last observation in interval.\n",
    "    agg_dict_lob = {}\n",
    "    if 'best_bid_price' in df_lob_raw.columns: agg_dict_lob['best_bid_price'] = 'last'\n",
    "    if 'best_bid_qty' in df_lob_raw.columns: agg_dict_lob['best_bid_qty'] = 'last'\n",
    "    if 'best_ask_price' in df_lob_raw.columns: agg_dict_lob['best_ask_price'] = 'last'\n",
    "    if 'best_ask_qty' in df_lob_raw.columns: agg_dict_lob['best_ask_qty'] = 'last'\n",
    "    for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "        if f'bid_price_L{i}' in df_lob_raw.columns: agg_dict_lob[f'bid_price_L{i}'] = 'last'\n",
    "        if f'bid_qty_L{i}' in df_lob_raw.columns: agg_dict_lob[f'bid_qty_L{i}'] = 'last'\n",
    "        if f'ask_price_L{i}' in df_lob_raw.columns: agg_dict_lob[f'ask_price_L{i}'] = 'last'\n",
    "        if f'ask_qty_L{i}' in df_lob_raw.columns: agg_dict_lob[f'ask_qty_L{i}'] = 'last'\n",
    "\n",
    "    if df_lob_raw.index.empty or not isinstance(df_lob_raw.index, pd.DatetimeIndex):\n",
    "        print(\"Warning: df_lob_raw has no DatetimeIndex. Cannot resample. Ensure 'timestamp' is index and datetime.\")\n",
    "        df_lob_sec = df_lob_raw.copy() # Or handle error appropriately\n",
    "    elif not agg_dict_lob:\n",
    "        print(\"Warning: No LOB columns found for aggregation in df_lob_raw.\")\n",
    "        df_lob_sec = pd.DataFrame(index=df_lob_raw.index.unique()) # Empty df with original index\n",
    "    else:\n",
    "        df_lob_sec = df_lob_raw.resample('1S').agg(agg_dict_lob)\n",
    "    \n",
    "    if 'best_bid_price' in df_lob_sec.columns and 'best_ask_price' in df_lob_sec.columns:\n",
    "        df_lob_sec.dropna(subset=['best_bid_price', 'best_ask_price'], inplace=True)\n",
    "    print(\"Resampled LOB data (1-second frequency, head):\\n\", df_lob_sec.head())\n",
    "else:\n",
    "    print(\"df_lob_raw is empty. Skipping LOB resampling.\")\n",
    "    df_lob_sec = pd.DataFrame()\n",
    "\n",
    "if not df_trades_raw.empty:\n",
    "    agg_dict_trades = {}\n",
    "    if 'price' in df_trades_raw.columns: agg_dict_trades['price'] = 'last' \n",
    "    if 'qty' in df_trades_raw.columns: agg_dict_trades['qty'] = 'sum'\n",
    "    if 'value_usd' in df_trades_raw.columns: agg_dict_trades['value_usd'] = 'sum'\n",
    "    \n",
    "    if df_trades_raw.index.empty or not isinstance(df_trades_raw.index, pd.DatetimeIndex):\n",
    "        print(\"Warning: df_trades_raw has no DatetimeIndex. Cannot resample.\")\n",
    "        df_trades_sec = df_trades_raw.copy()\n",
    "        buy_volume_sec = pd.Series(name='buy_qty_sum', dtype='float64')\n",
    "        sell_volume_sec = pd.Series(name='sell_qty_sum', dtype='float64')\n",
    "        buy_value_sec = pd.Series(name='buy_value_sum', dtype='float64')\n",
    "        sell_value_sec = pd.Series(name='sell_value_sum', dtype='float64')\n",
    "    elif not agg_dict_trades:\n",
    "        print(\"Warning: No trade columns for aggregation in df_trades_raw.\")\n",
    "        df_trades_sec = pd.DataFrame(index=df_trades_raw.index.unique() if isinstance(df_trades_raw.index, pd.DatetimeIndex) else None)\n",
    "        buy_volume_sec = pd.Series(name='buy_qty_sum', dtype='float64')\n",
    "        sell_volume_sec = pd.Series(name='sell_qty_sum', dtype='float64')\n",
    "        buy_value_sec = pd.Series(name='buy_value_sum', dtype='float64')\n",
    "        sell_value_sec = pd.Series(name='sell_value_sum', dtype='float64')\n",
    "    else:\n",
    "        df_trades_sec = df_trades_raw.resample('1S').agg(agg_dict_trades)\n",
    "        # Ensure 'side' column exists before trying to filter by it\n",
    "        if 'side' in df_trades_raw.columns:\n",
    "            buy_volume_sec = df_trades_raw[df_trades_raw['side'].str.lower() == 'buy']['qty'].resample('1S').sum().rename('buy_qty_sum')\n",
    "            sell_volume_sec = df_trades_raw[df_trades_raw['side'].str.lower() == 'sell']['qty'].resample('1S').sum().rename('sell_qty_sum')\n",
    "            buy_value_sec = df_trades_raw[df_trades_raw['side'].str.lower() == 'buy']['value_usd'].resample('1S').sum().rename('buy_value_sum')\n",
    "            sell_value_sec = df_trades_raw[df_trades_raw['side'].str.lower() == 'sell']['value_usd'].resample('1S').sum().rename('sell_value_sum')\n",
    "        else:\n",
    "            print(\"Warning: 'side' column missing in df_trades_raw. Cannot calculate buy/sell specific sums.\")\n",
    "            buy_volume_sec = pd.Series(name='buy_qty_sum', index=df_trades_sec.index, dtype='float64').fillna(0)\n",
    "            sell_volume_sec = pd.Series(name='sell_qty_sum', index=df_trades_sec.index, dtype='float64').fillna(0)\n",
    "            buy_value_sec = pd.Series(name='buy_value_sum', index=df_trades_sec.index, dtype='float64').fillna(0)\n",
    "            sell_value_sec = pd.Series(name='sell_value_sum', index=df_trades_sec.index, dtype='float64').fillna(0)\n",
    "\n",
    "    df_trades_info_sec = pd.concat([df_trades_sec, buy_volume_sec, sell_volume_sec, buy_value_sec, sell_value_sec], axis=1)\n",
    "    df_trades_info_sec.fillna({'buy_qty_sum': 0, 'sell_qty_sum': 0, 'buy_value_sum':0, 'sell_value_sum':0}, inplace=True)\n",
    "    print(\"Resampled Trades data (1-second frequency, head):\\n\", df_trades_info_sec.head())\n",
    "else:\n",
    "    print(\"df_trades_raw is empty. Skipping trades resampling.\")\n",
    "    df_trades_info_sec = pd.DataFrame()\n",
    "\n",
    "df_combined = pd.DataFrame()\n",
    "if not df_lob_sec.empty and not df_trades_info_sec.empty:\n",
    "    df_combined = pd.merge(df_lob_sec, df_trades_info_sec, left_index=True, right_index=True, how='outer')\n",
    "elif not df_lob_sec.empty:\n",
    "    df_combined = df_lob_sec\n",
    "elif not df_trades_info_sec.empty:\n",
    "    df_combined = df_trades_info_sec\n",
    "\n",
    "if not df_combined.empty:\n",
    "    if 'best_bid_price' in df_combined.columns: \n",
    "        lob_cols = df_lob_sec.columns if not df_lob_sec.empty else []\n",
    "        lob_cols_to_ffill = [col for col in lob_cols if col in df_combined.columns]\n",
    "        if lob_cols_to_ffill:\n",
    "             df_combined[lob_cols_to_ffill] = df_combined[lob_cols_to_ffill].ffill().bfill()\n",
    "\n",
    "    trade_qty_val_cols = ['qty', 'value_usd', 'buy_qty_sum', 'sell_qty_sum', 'buy_value_sum', 'sell_value_sum']\n",
    "    for col in trade_qty_val_cols:\n",
    "        if col in df_combined.columns:\n",
    "            df_combined[col].fillna(0, inplace=True)\n",
    "    \n",
    "    if 'best_bid_price' in df_combined.columns and 'best_ask_price' in df_combined.columns:\n",
    "        df_combined.dropna(subset=['best_bid_price', 'best_ask_price'], inplace=True)\n",
    "    else:\n",
    "        # If essential columns are missing after merge, df_combined might not be usable\n",
    "        print(\"Warning: Essential LOB columns (best_bid_price, best_ask_price) are missing in df_combined. Further steps might fail.\")\n",
    "        # df_combined = pd.DataFrame() # Or decide to stop / handle differently\n",
    "    print(\"Combined and preprocessed data (head):\\n\", df_combined.head())\n",
    "else:\n",
    "    print(\"df_combined is empty after merging/resampling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bf7c3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering\n",
    "Based on the paper \"Mind the Gaps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have the necessary base columns\n",
    "if df_combined.empty or not all(col in df_combined.columns for col in ['best_bid_price', 'best_ask_price']):\n",
    "    print(\"Skipping feature engineering due to missing base LOB data (best_bid_price, best_ask_price) in df_combined.\")\n",
    "else:\n",
    "    # --- Mid-Price and Spread ---\n",
    "    df_combined['mid_price'] = (df_combined['best_bid_price'] + df_combined['best_ask_price']) / 2\n",
    "    df_combined['spread'] = df_combined['best_ask_price'] - df_combined['best_bid_price']\n",
    "\n",
    "    # --- Volume-Adjusted Mid-Price (VAMP) ---\n",
    "    def calculate_weighted_price_for_vamp(levels_prices, levels_qty, target_value_usd):\n",
    "        \"\"\" Helper for VAMP. Calculates weighted price for one side. \"\"\"\n",
    "        cumulative_value = 0\n",
    "        weighted_price_sum = 0\n",
    "        total_qty_for_value = 0\n",
    "        for price, qty in zip(levels_prices, levels_qty):\n",
    "            if pd.isna(price) or pd.isna(qty) or qty == 0 or price == 0: # Added price == 0 check\n",
    "                continue\n",
    "            value_at_level = price * qty\n",
    "            if cumulative_value + value_at_level >= target_value_usd:\n",
    "                remaining_value_needed = target_value_usd - cumulative_value\n",
    "                if price == 0: continue # Avoid division by zero if price is somehow zero\n",
    "                qty_to_take = remaining_value_needed / price\n",
    "                weighted_price_sum += price * qty_to_take\n",
    "                total_qty_for_value += qty_to_take\n",
    "                cumulative_value += remaining_value_needed\n",
    "                break\n",
    "            else:\n",
    "                weighted_price_sum += price * qty \n",
    "                total_qty_for_value += qty\n",
    "                cumulative_value += value_at_level\n",
    "        return (weighted_price_sum / total_qty_for_value) if total_qty_for_value > 0 else np.nan\n",
    "\n",
    "    vamp_bids_p_cols = [f'bid_price_L{i}' for i in range(1, QUOTE_IMBALANCE_LEVELS + 1) if f'bid_price_L{i}' in df_combined.columns]\n",
    "    vamp_bids_q_cols = [f'bid_qty_L{i}' for i in range(1, QUOTE_IMBALANCE_LEVELS + 1) if f'bid_qty_L{i}' in df_combined.columns]\n",
    "    vamp_asks_p_cols = [f'ask_price_L{i}' for i in range(1, QUOTE_IMBALANCE_LEVELS + 1) if f'ask_price_L{i}' in df_combined.columns]\n",
    "    vamp_asks_q_cols = [f'ask_qty_L{i}' for i in range(1, QUOTE_IMBALANCE_LEVELS + 1) if f'ask_qty_L{i}' in df_combined.columns]\n",
    "\n",
    "    if vamp_bids_p_cols and vamp_bids_q_cols and vamp_asks_p_cols and vamp_asks_q_cols and \\\n",
    "       len(vamp_bids_p_cols) == len(vamp_bids_q_cols) and len(vamp_asks_p_cols) == len(vamp_asks_q_cols):\n",
    "        \n",
    "        Pb_vamp_values = []\n",
    "        Pa_vamp_values = []\n",
    "        for index, row in df_combined.iterrows():\n",
    "            bid_prices = [row[col] for col in vamp_bids_p_cols]\n",
    "            bid_qtys = [row[col] for col in vamp_bids_q_cols]\n",
    "            ask_prices = [row[col] for col in vamp_asks_p_cols]\n",
    "            ask_qtys = [row[col] for col in vamp_asks_q_cols]\n",
    "            \n",
    "            Pb_vamp_values.append(calculate_weighted_price_for_vamp(bid_prices, bid_qtys, VAMP_LIQUIDITY_CUTOFF))\n",
    "            Pa_vamp_values.append(calculate_weighted_price_for_vamp(ask_prices, ask_qtys, VAMP_LIQUIDITY_CUTOFF))\n",
    "            \n",
    "        df_combined['Pb_vamp'] = Pb_vamp_values\n",
    "        df_combined['Pa_vamp'] = Pa_vamp_values\n",
    "        df_combined['vamp'] = (df_combined['Pb_vamp'] + df_combined['Pa_vamp']) / 2\n",
    "        df_combined['vamp_mid_diff'] = df_combined['mid_price'] - df_combined['vamp']\n",
    "    else:\n",
    "        df_combined['vamp'] = df_combined['mid_price']\n",
    "        df_combined['vamp_mid_diff'] = 0\n",
    "        print(\"Warning: VAMP calculation could not be performed due to missing/mismatched deep LOB data. Using mid_price as fallback.\")\n",
    "\n",
    "    # --- Trade Imbalance (TI) ---\n",
    "    if 'buy_value_sum' in df_combined.columns and 'sell_value_sum' in df_combined.columns:\n",
    "        rolling_window_size_ti = int(pd.Timedelta(TRADE_IMBALANCE_WINDOW).total_seconds())\n",
    "        diff_value = df_combined['buy_value_sum'] - df_combined['sell_value_sum']\n",
    "        total_value = df_combined['buy_value_sum'] + df_combined['sell_value_sum']\n",
    "        numerator_ti = diff_value.rolling(window=rolling_window_size_ti, min_periods=max(1, int(rolling_window_size_ti*0.1))).sum() # Ensure min_periods is at least 1\n",
    "        denominator_ti = total_value.rolling(window=rolling_window_size_ti, min_periods=max(1, int(rolling_window_size_ti*0.1))).sum()\n",
    "        df_combined['trade_imbalance'] = (numerator_ti / denominator_ti.replace(0, np.nan)).fillna(0) \n",
    "        df_combined['trade_imbalance'] = np.clip(df_combined['trade_imbalance'], -1, 1)\n",
    "    else:\n",
    "        df_combined['trade_imbalance'] = 0\n",
    "        print(\"Warning: Trade Imbalance calculation skipped due to missing trade value columns.\")\n",
    "\n",
    "    # --- Quote Imbalance (QI) ---\n",
    "    sum_bid_qty_L = pd.Series(0.0, index=df_combined.index)\n",
    "    sum_ask_qty_L = pd.Series(0.0, index=df_combined.index)\n",
    "    qi_cols_found = False\n",
    "    for i in range(1, QUOTE_IMBALANCE_LEVELS + 1):\n",
    "        bid_col_name = f'bid_qty_L{i}'\n",
    "        ask_col_name = f'ask_qty_L{i}'\n",
    "        if bid_col_name in df_combined.columns and ask_col_name in df_combined.columns:\n",
    "            sum_bid_qty_L += df_combined[bid_col_name].fillna(0)\n",
    "            sum_ask_qty_L += df_combined[ask_col_name].fillna(0)\n",
    "            qi_cols_found = True\n",
    "            \n",
    "    if qi_cols_found:\n",
    "        numerator_qi = sum_bid_qty_L - sum_ask_qty_L\n",
    "        denominator_qi = sum_bid_qty_L + sum_ask_qty_L\n",
    "        df_combined['quote_imbalance'] = (numerator_qi / denominator_qi.replace(0, np.nan)).fillna(0)\n",
    "        df_combined['quote_imbalance'] = np.clip(df_combined['quote_imbalance'], -1, 1)\n",
    "    else:\n",
    "        df_combined['quote_imbalance'] = 0\n",
    "        print(\"Warning: Quote Imbalance calculation skipped due to missing LOB quantity columns.\")\n",
    "\n",
    "    print(\"Data with engineered features (head):\\n\", df_combined[['mid_price', 'spread', 'vamp', 'vamp_mid_diff', 'trade_imbalance', 'quote_imbalance']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c5218",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Target Variable Creation\n",
    "We want to predict future price changes. The PDF uses look-ahead windows from 1s to 60s.\n",
    "Let's use `PREDICTION_HORIZON_SECONDS`.\n",
    "Target: $MidPrice_{t+\\Delta t} - MidPrice_t$ (absolute change) or % change.\n",
    "Or, for classification: sign of change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d83ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mid_price' not in df_combined.columns or df_combined.empty:\n",
    "    print(\"Skipping target variable creation as mid_price is missing or df_combined is empty.\")\n",
    "    df_final = pd.DataFrame() # Ensure df_final exists\n",
    "else:\n",
    "    df_combined['future_mid_price'] = df_combined['mid_price'].shift(-PREDICTION_HORIZON_SECONDS)\n",
    "    \n",
    "    if TARGET_TYPE == 'regression':\n",
    "        df_combined['target_price_change'] = df_combined['future_mid_price'] - df_combined['mid_price']\n",
    "        TARGET_COLUMN = 'target_price_change'\n",
    "    elif TARGET_TYPE == 'classification':\n",
    "        price_diff = df_combined['future_mid_price'] - df_combined['mid_price']\n",
    "        # Ensure spread is available and not all NaNs before calculating mean\n",
    "        if 'spread' in df_combined.columns and not df_combined['spread'].isnull().all():\n",
    "            neutral_threshold = df_combined['spread'].mean() * 0.1 \n",
    "        else:\n",
    "            neutral_threshold = 0.001 # Fallback if spread is not available\n",
    "            print(f\"Warning: 'spread' column missing or all NaN. Using fixed neutral_threshold: {neutral_threshold}\")\n",
    "        df_combined['target_direction'] = 1 # Neutral\n",
    "        df_combined.loc[price_diff > neutral_threshold, 'target_direction'] = 2 # Up\n",
    "        df_combined.loc[price_diff < -neutral_threshold, 'target_direction'] = 0 # Down\n",
    "        TARGET_COLUMN = 'target_direction'\n",
    "\n",
    "    df_final = df_combined.dropna(subset=[TARGET_COLUMN])\n",
    "    \n",
    "    if not df_final.empty:\n",
    "        print(f\"Final data with target variable '{TARGET_COLUMN}' (head):\\n\", df_final[['mid_price', 'future_mid_price', TARGET_COLUMN] + [col for col in ['vamp_mid_diff', 'trade_imbalance', 'quote_imbalance'] if col in df_final.columns]].head())\n",
    "        print(f\"\\nTarget variable ({TARGET_COLUMN}) distribution:\")\n",
    "        if TARGET_TYPE == 'regression':\n",
    "            df_final[TARGET_COLUMN].plot(kind='hist', bins=50, title='Target Price Change Distribution')\n",
    "            plt.show()\n",
    "            print(df_final[TARGET_COLUMN].describe())\n",
    "        elif TARGET_TYPE == 'classification':\n",
    "            print(df_final[TARGET_COLUMN].value_counts(normalize=True))\n",
    "    else:\n",
    "        print(\"DataFrame is empty after target creation and NaN removal. Check data or prediction horizon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7b6a8",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Stationarity Checks and Transformations\n",
    "Time series data for financial modeling often requires features to be stationary.\n",
    "The target variable (price change or returns) is usually stationary.\n",
    "We should check engineered features like `vamp_mid_diff`, `trade_imbalance`, `quote_imbalance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f49cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(series, series_name=''):\n",
    "    \"\"\"Performs ADF test and prints results.\"\"\"\n",
    "    if not isinstance(series, pd.Series) or series.empty or series.isnull().all():\n",
    "        print(f\"Series {series_name} is not a valid Series, is empty, or all NaN. Skipping stationarity check.\")\n",
    "        return False \n",
    "    print(f'\\nStationarity Test for {series_name}:')\n",
    "    try:\n",
    "        # Ensure data is float for adfuller, and dropna\n",
    "        series_cleaned = series.dropna().astype(float)\n",
    "        if series_cleaned.empty:\n",
    "            print(f\"Series {series_name} is empty after dropna. Skipping stationarity check.\")\n",
    "            return False\n",
    "        result = adfuller(series_cleaned) \n",
    "        print('ADF Statistic: %f' % result[0])\n",
    "        print('p-value: %f' % result[1])\n",
    "        if result[1] <= 0.05:\n",
    "            print(f\"Result: Likely Stationary (p-value <= 0.05) for {series_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Result: Likely Non-Stationary (p-value > 0.05) for {series_name}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error during stationarity test for {series_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "feature_columns_for_model = []\n",
    "df_model_ready = pd.DataFrame() \n",
    "\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    potential_features = ['spread', 'vamp_mid_diff', 'trade_imbalance', 'quote_imbalance']\n",
    "    df_transformed = df_final.copy()\n",
    "\n",
    "    for col in potential_features:\n",
    "        if col in df_transformed.columns:\n",
    "            if not check_stationarity(df_transformed[col], col):\n",
    "                print(f\"Feature {col} is non-stationary. Applying differencing.\")\n",
    "                df_transformed[f'{col}_diff'] = df_transformed[col].diff()\n",
    "                # Check stationarity of the differenced series\n",
    "                if check_stationarity(df_transformed[f'{col}_diff'].dropna(), f'{col}_diff'):\n",
    "                    feature_columns_for_model.append(f'{col}_diff')\n",
    "                else:\n",
    "                    print(f\"Differenced feature {col}_diff is still non-stationary. Consider further transformation or excluding.\")\n",
    "                    # Optionally, still add the differenced feature if you want to proceed with caution\n",
    "                    # feature_columns_for_model.append(f'{col}_diff') \n",
    "            else:\n",
    "                feature_columns_for_model.append(col)\n",
    "    \n",
    "    if TARGET_TYPE == 'regression' and TARGET_COLUMN in df_transformed.columns:\n",
    "        check_stationarity(df_transformed[TARGET_COLUMN], TARGET_COLUMN)\n",
    "    \n",
    "    if feature_columns_for_model and TARGET_COLUMN in df_transformed.columns:\n",
    "        # Ensure all selected feature columns and the target column exist before creating df_model_ready\n",
    "        final_cols_to_select = [f for f in feature_columns_for_model if f in df_transformed.columns] + [TARGET_COLUMN]\n",
    "        df_model_ready = df_transformed[final_cols_to_select].copy()\n",
    "        df_model_ready.dropna(inplace=True)\n",
    "    else:\n",
    "        print(\"Warning: No features selected or target column missing after stationarity check.\")\n",
    "\n",
    "    if df_model_ready.empty:\n",
    "        print(\"DataFrame for modeling (df_model_ready) is empty after dropping NaNs. Check data, feature engineering, and stationarity steps.\")\n",
    "    else:\n",
    "        print(f\"\\nFeatures selected for modeling: {feature_columns_for_model}\")\n",
    "        print(f\"Shape of df_model_ready: {df_model_ready.shape}\")\n",
    "else:\n",
    "    print(\"Skipping stationarity checks as df_final is not available or empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8efd1",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_model_ready' not in locals() or df_model_ready.empty or not feature_columns_for_model:\n",
    "    print(\"Skipping model building as data is not ready or no features are selected.\")\n",
    "else:\n",
    "    X = df_model_ready[feature_columns_for_model]\n",
    "    y = df_model_ready[TARGET_COLUMN]\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(\"X or y is empty. Cannot proceed with model training.\")\n",
    "    else:\n",
    "        train_size_pct = 0.8\n",
    "        split_idx = int(len(X) * train_size_pct)\n",
    "        \n",
    "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "        if X_train.empty or X_test.empty:\n",
    "            print(\"Training or testing set is empty after split. Check data size and split point.\")\n",
    "        else:\n",
    "            print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model_results = {}\n",
    "\n",
    "            def evaluate_model(name, model, X_test_data, y_true, y_pred):\n",
    "                if TARGET_TYPE == 'regression':\n",
    "                    mse = mean_squared_error(y_true, y_pred)\n",
    "                    r2 = r2_score(y_true, y_pred)\n",
    "                    print(f\"{name} - MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "                    model_results[name] = {'MSE': mse, 'R2': r2}\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.scatter(y_true, y_pred, alpha=0.5, label='Predicted vs Actual')\n",
    "                    min_val = min(y_true.min(), y_pred.min()) if not y_true.empty and not pd.Series(y_pred).empty else 0\n",
    "                    max_val = max(y_true.max(), y_pred.max()) if not y_true.empty and not pd.Series(y_pred).empty else 1\n",
    "                    plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')\n",
    "                    plt.xlabel(\"Actual Values\")\n",
    "                    plt.ylabel(\"Predicted Values\")\n",
    "                    plt.title(f\"{name} - Predictions vs Actuals\")\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "                elif TARGET_TYPE == 'classification':\n",
    "                    accuracy = accuracy_score(y_true, y_pred)\n",
    "                    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "                    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "                    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "                    print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "                    model_results[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1}\n",
    "\n",
    "            # --- Model Implementations ---\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                print(\"\\n--- Linear Regression ---\")\n",
    "                model_lr = LinearRegression()\n",
    "                model_lr.fit(X_train_scaled, y_train)\n",
    "                y_pred_lr = model_lr.predict(X_test_scaled)\n",
    "                evaluate_model(\"Linear Regression\", model_lr, X_test_scaled, y_test, y_pred_lr)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                print(\"\\n--- Logistic Regression ---\")\n",
    "                model_logr = LogisticRegression(solver='liblinear', multi_class='auto', random_state=42, max_iter=1000)\n",
    "                model_logr.fit(X_train_scaled, y_train)\n",
    "                y_pred_logr = model_logr.predict(X_test_scaled)\n",
    "                evaluate_model(\"Logistic Regression\", model_logr, X_test_scaled, y_test, y_pred_logr)\n",
    "\n",
    "            print(\"\\n--- Decision Tree ---\")\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_dt = DecisionTreeRegressor(random_state=42, max_depth=10, min_samples_split=10)\n",
    "                model_dt.fit(X_train_scaled, y_train)\n",
    "                y_pred_dt = model_dt.predict(X_test_scaled)\n",
    "                evaluate_model(\"Decision Tree Regressor\", model_dt, X_test_scaled, y_test, y_pred_dt)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                model_dtc = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=10)\n",
    "                model_dtc.fit(X_train_scaled, y_train)\n",
    "                y_pred_dtc = model_dtc.predict(X_test_scaled)\n",
    "                evaluate_model(\"Decision Tree Classifier\", model_dtc, X_test_scaled, y_test, y_pred_dtc)\n",
    "\n",
    "            print(\"\\n--- Random Forest ---\")\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10, n_jobs=-1)\n",
    "                model_rf.fit(X_train_scaled, y_train)\n",
    "                y_pred_rf = model_rf.predict(X_test_scaled)\n",
    "                evaluate_model(\"Random Forest Regressor\", model_rf, X_test_scaled, y_test, y_pred_rf)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                model_rfc = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10, n_jobs=-1)\n",
    "                model_rfc.fit(X_train_scaled, y_train)\n",
    "                y_pred_rfc = model_rfc.predict(X_test_scaled)\n",
    "                evaluate_model(\"Random Forest Classifier\", model_rfc, X_test_scaled, y_test, y_pred_rfc)\n",
    "\n",
    "            print(\"\\n--- XGBoost ---\")\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42, max_depth=7, learning_rate=0.1, n_jobs=-1)\n",
    "                model_xgb_reg.fit(X_train_scaled, y_train)\n",
    "                y_pred_xgb_reg = model_xgb_reg.predict(X_test_scaled)\n",
    "                evaluate_model(\"XGBoost Regressor\", model_xgb_reg, X_test_scaled, y_test, y_pred_xgb_reg)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                num_class_xgb = len(np.unique(y_train)) \n",
    "                objective_xgb_clf = 'multi:softmax' if num_class_xgb > 2 else 'binary:logistic'\n",
    "                model_xgbc_params = {'n_estimators': 100, 'random_state': 42, 'max_depth': 7, 'learning_rate': 0.1, 'n_jobs': -1, 'objective': objective_xgb_clf}\n",
    "                if objective_xgb_clf == 'multi:softmax': model_xgbc_params['num_class'] = num_class_xgb\n",
    "                model_xgbc = xgb.XGBClassifier(**model_xgbc_params)\n",
    "                model_xgbc.fit(X_train_scaled, y_train)\n",
    "                y_pred_xgbc = model_xgbc.predict(X_test_scaled)\n",
    "                evaluate_model(\"XGBoost Classifier\", model_xgbc, X_test_scaled, y_test, y_pred_xgbc)\n",
    "                \n",
    "            print(\"\\n--- Support Vector Machine (SVM) ---\")\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "                model_svr.fit(X_train_scaled, y_train)\n",
    "                y_pred_svr = model_svr.predict(X_test_scaled)\n",
    "                evaluate_model(\"SVR\", model_svr, X_test_scaled, y_test, y_pred_svr)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                model_svc = SVC(kernel='rbf', C=1.0, random_state=42, probability=True)\n",
    "                model_svc.fit(X_train_scaled, y_train)\n",
    "                y_pred_svc = model_svc.predict(X_test_scaled)\n",
    "                evaluate_model(\"SVC\", model_svc, X_test_scaled, y_test, y_pred_svc)\n",
    "\n",
    "            print(\"\\n--- Neural Network (MLP) ---\")\n",
    "            def create_mlp(input_dim, num_unique_targets=1, classification=False):\n",
    "                model = Sequential()\n",
    "                model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(32, activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                if classification:\n",
    "                    if num_unique_targets <= 2: \n",
    "                         model.add(Dense(1, activation='sigmoid'))\n",
    "                         model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                    else: \n",
    "                         model.add(Dense(num_unique_targets, activation='softmax'))\n",
    "                         model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                else: \n",
    "                    model.add(Dense(1, activation='linear'))\n",
    "                    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "                return model\n",
    "\n",
    "            if TARGET_TYPE == 'regression':\n",
    "                model_mlp_reg = create_mlp(X_train_scaled.shape[1], classification=False)\n",
    "                early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "                model_mlp_reg.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stop], verbose=0)\n",
    "                y_pred_mlp_reg = model_mlp_reg.predict(X_test_scaled).flatten()\n",
    "                evaluate_model(\"MLP Regressor\", model_mlp_reg, X_test_scaled, y_test, y_pred_mlp_reg)\n",
    "            elif TARGET_TYPE == 'classification':\n",
    "                num_unique_targets_nn = len(np.unique(y_train))\n",
    "                model_mlpc = create_mlp(X_train_scaled.shape[1], num_unique_targets=num_unique_targets_nn, classification=True)\n",
    "                early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "                model_mlpc.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stop], verbose=0)\n",
    "                y_pred_mlpc_proba = model_mlpc.predict(X_test_scaled)\n",
    "                if num_unique_targets_nn <= 2: \n",
    "                    y_pred_mlpc = (y_pred_mlpc_proba > 0.5).astype(int).flatten()\n",
    "                else: \n",
    "                    y_pred_mlpc = np.argmax(y_pred_mlpc_proba, axis=1)\n",
    "                evaluate_model(\"MLP Classifier\", model_mlpc, X_test_scaled, y_test, y_pred_mlpc)\n",
    "\n",
    "            print(\"\\n--- Model Performance Summary ---\")\n",
    "            results_df = pd.DataFrame(model_results).T\n",
    "            print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Conclusion and Future Work\n",
    "\n",
    "This notebook provided a framework for acquiring, preprocessing, and modeling cryptocurrency order book data for short-term price prediction. Key features inspired by \"Mind the Gaps\" such as Mid-Price, Spread, VAMP, Trade Imbalance, and Quote Imbalance were implemented.\n",
    "\n",
    "**Observations from this run (based on sample data/placeholder logic):**\n",
    "* (Actual observations will depend on the real data and model performance)\n",
    "* The VAMP feature was noted in the paper as a strong predictor. Its effectiveness would depend on the quality and depth of LOB data used.\n",
    "* Trade Imbalance and Quote Imbalance aim to capture market pressure.\n",
    "\n",
    "**Future Work:**\n",
    "* **Robust Data Pipeline**: Implement a more robust data acquisition pipeline, especially for live API data (e.g., using WebSockets for continuous LOB updates and reconstruction).\n",
    "* **Advanced Feature Engineering**:\n",
    "    * Explore more sophisticated weighting for Trade Imbalance.\n",
    "    * Test different VAMP liquidity cutoffs and QI levels systematically.\n",
    "    * Incorporate features like realized volatility, order flow toxicity, or market impact models.\n",
    "* **Hyperparameter Tuning**: Systematically tune hyperparameters for each ML model (e.g., using GridSearchCV or RandomizedSearchCV with TimeSeriesSplit).\n",
    "* **Stationarity**: Rigorously ensure all features used in models are stationary. Apply transformations like differencing if needed and re-evaluate.\n",
    "* **Model Ensembling/Stacking**: Combine predictions from multiple models to potentially improve performance.\n",
    "* **Deeper Neural Networks**: Explore more complex architectures like LSTMs or GRUs, which are well-suited for time series data.\n",
    "* **Alternative Prediction Targets**: Expand on the binary and multiclass classification approaches from the paper, especially predicting one-standard-deviation price movements.\n",
    "* **Backtesting Framework**: Develop a rigorous backtesting framework that accounts for transaction costs, slippage, and realistic trading conditions. The P&L metric in the paper is a good starting point.\n",
    "* **Expand Dataset**: Analyze data across different crypto assets and exchanges, and longer time periods, including diverse market conditions (e.g., high volatility periods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### References from \"Mind the Gaps\" used in this notebook:\n",
    "- [1] Martin, P., Line Jr., W., Feng, Y., Yang, Y., Zheng, S., Qi, S., & Zhu, B. (2022). *Mind the Gaps: Short-term Crypto Price Prediction*. Cornell University. Available at SSRN: https://ssrn.com/abstract=4351947\n",
    "- [16] Prediction at time scales from one second to 60 seconds.\n",
    "- [18] Volume-Adjusted Mid-Price as the ultimate short-term predictor.\n",
    "- [19, 20] Data sourcing: Bitstamp, three full months, tick level.\n",
    "- [21] Initial feature calculation: spread, mid-price, best bid/ask, volume-adjusted versions.\n",
    "- [25, 26] Condensing dataset to full seconds.\n",
    "- [48, 49] Volume-Adjusted Mid-Price (VAMP) definition and formula.\n",
    "- [52] Plotting (mid-price - VAMP) against returns.\n",
    "- [54, 55, 159] VAMP volume cutoffs, settling on $50k-$60k range, specifically $60k.\n",
    "- [56, 57, 58] Trade Imbalance (TI) definition, formula with linear weight, range -1 to 1.\n",
    "- [72] Using 1-minute window for Trade Imbalance.\n",
    "- [88, 89, 94] Quote Imbalance (QI) definition, formula, range -1 to 1, using up to level 5.\n",
    "- [93] QI relationship becoming more linear with deeper levels.\n",
    "- [128] Trading P&L metric introduction.\n",
    "- [152] Binary classification setup: strict inequalities for price change prediction.\n",
    "- [171] Multiclass classification setup: one standard deviation thresholds.\n",
    "- [195] Expanding data to include diverse BTC data and volatile conditions.\n",
    "\n",
    "Note: Citation numbers in the markdown cells (e.g., `[cite: X]`) refer to page numbers or specific findings in the provided PDF \"Mind-the-Gaps-Short-term-Crypto-Price-Prediction-2022.pdf\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
